{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "from random import randint, random\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_movies = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/movies.csv\"\n",
    "p_countries = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/countries.csv\"\n",
    "p_genres = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/genres.csv\"\n",
    "p_staff = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/staff.csv\"\n",
    "p_logs = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/logs.csv\"\n",
    "'''\n",
    "p_movies = \"C:/Users/Vergas\\Contest2023/train/movies.csv\"\n",
    "p_countries = \"C:/Users/Vergas/Contest2023/train/countries.csv\"\n",
    "p_genres = \"C:/Users/Vergas/Contest2023/train/genres.csv\"\n",
    "p_staff = \"C:/Users/Vergas/Contest2023/train/staff.csv\"\n",
    "p_logs = \"C:/Users/Vergas/Contest2023/train/logs.csv\"\n",
    "'''\n",
    "movies = pd.read_csv(p_movies)\n",
    "countries = pd.read_csv(p_countries)\n",
    "genres = pd.read_csv(p_genres)\n",
    "staff = pd.read_csv(p_staff)\n",
    "logs = pd.read_csv(p_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаляем из списка фильмо те, которые не были опубликованы\n",
    "movies = movies[~movies['date_publication'].isnull()]\n",
    "# переводим тип float to intager\n",
    "logs['movie_id'] = logs['movie_id'].astype(int)\n",
    "logs['duration'] = logs['duration'].astype(int)\n",
    "logs['datetime'] = pd.to_datetime(logs['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nprint(f\"min date in filtered interactions: {MAX_DATE}\")\\nprint(f\"max date in filtered interactions:: {MIN_DATE}\")\\nprint(f\"test max date to split:: {TEST_MAX_DATE}\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set dates params for filter\n",
    "MAX_DATE = logs['datetime'].max()\n",
    "MIN_DATE = logs['datetime'].min()\n",
    "TEST_INTERVAL_DAYS = 14\n",
    "TEST_MAX_DATE = MAX_DATE - dt.timedelta(days = TEST_INTERVAL_DAYS)\n",
    "''' \n",
    "print(f\"min date in filtered interactions: {MAX_DATE}\")\n",
    "print(f\"max date in filtered interactions:: {MIN_DATE}\")\n",
    "print(f\"test max date to split:: {TEST_MAX_DATE}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_test = logs.loc[logs['datetime'] >= TEST_MAX_DATE]\n",
    "logs_test = logs_test[[\"user_id\", \"movie_id\"]].drop_duplicates().rename(columns={'movie_id':'item_id'})\n",
    "logs_test['interaction'] = 1\n",
    "logs_train = logs.loc[logs['datetime'] < TEST_MAX_DATE]\n",
    "logs_test = logs_test[logs_test['user_id'].isin(logs_train['user_id'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_movie = list(logs_train['movie_id'].value_counts().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_u(uid, l_test, pop_movie):\n",
    "    watched = list(l_test[l_test['user_id']==uid].item_id)\n",
    "    pop_movie = [x for x in pop_movie if x not in watched][:20]\n",
    "    rank_df = pd.DataFrame({\n",
    "        'user_id': np.repeat(uid, 20),\n",
    "        'item_id': pop_movie,\n",
    "        'rank': np.arange(1, 21)\n",
    "    })\n",
    "    return rank_df\n",
    "\n",
    "\n",
    "bl_pred = pd.concat([pd.DataFrame(top_u(x, logs_test, pop_movie)) for x in logs_test['user_id'].unique()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_quantile_(0.0, 45743.0]</th>\n",
       "      <th>movie_quantile_(45743.0, 91485.0]</th>\n",
       "      <th>movie_quantile_(91485.0, 137227.0]</th>\n",
       "      <th>movie_quantile_(137227.0, 182969.0]</th>\n",
       "      <th>duration_quantile_(0.0, 45743.0]</th>\n",
       "      <th>duration_quantile_(45743.0, 91485.0]</th>\n",
       "      <th>duration_quantile_(91485.0, 137227.0]</th>\n",
       "      <th>duration_quantile_(137227.0, 182969.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_quantile_(0.0, 45743.0]  movie_quantile_(45743.0, 91485.0]  \\\n",
       "0        0                              0                                  0   \n",
       "1        1                              0                                  0   \n",
       "2        2                              0                                  0   \n",
       "3        3                              0                                  0   \n",
       "4        4                              0                                  0   \n",
       "\n",
       "   movie_quantile_(91485.0, 137227.0]  movie_quantile_(137227.0, 182969.0]  \\\n",
       "0                                   0                                    1   \n",
       "1                                   0                                    1   \n",
       "2                                   0                                    1   \n",
       "3                                   0                                    1   \n",
       "4                                   0                                    1   \n",
       "\n",
       "   duration_quantile_(0.0, 45743.0]  duration_quantile_(45743.0, 91485.0]  \\\n",
       "0                                 0                                     0   \n",
       "1                                 0                                     0   \n",
       "2                                 0                                     0   \n",
       "3                                 0                                     0   \n",
       "4                                 0                                     0   \n",
       "\n",
       "   duration_quantile_(91485.0, 137227.0]  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   duration_quantile_(137227.0, 182969.0]  \n",
       "0                                       1  \n",
       "1                                       1  \n",
       "2                                       1  \n",
       "3                                       1  \n",
       "4                                       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим фичи пользователей\n",
    "users_df = logs_train.groupby('user_id', as_index=False).agg({'movie_id':'nunique', 'duration':'sum'}).rename(columns={'movie_id':'movie_count', 'duration':'sum_duration'})\n",
    "users_df['movie_quantile']   =pd.qcut(users_df['movie_count'].rank(method='first'), q=4, precision=0)\n",
    "users_df['duration_quantile']=pd.qcut(users_df['sum_duration'].rank(method='first'), q=4, precision=0)\n",
    "user_cat_feats = [\"movie_quantile\", \"duration_quantile\"]\n",
    "\n",
    "# из исходного датафрейма оставим только item_id - этот признак нам понадобится позже\n",
    "# для того, чтобы маппить айтемы из датафрейма с фильмами с айтемами \n",
    "# из датафрейма с взаимодействиями\n",
    "users_ohe_df = users_df.user_id\n",
    "for feat in user_cat_feats:\n",
    "  # получаем датафрейм с one-hot encoding для каждой категориальной фичи\n",
    "  ohe_feat_df = pd.get_dummies(users_df[feat], prefix=feat)\n",
    "  # конкатенируем ohe-hot датафрейм с датафреймом, \n",
    "  # который мы получили на предыдущем шаге\n",
    "  users_ohe_df = pd.concat([users_ohe_df, ohe_feat_df], axis=1)\n",
    "\n",
    "users_ohe_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>year_1895-01-01</th>\n",
       "      <th>year_1906-01-01</th>\n",
       "      <th>year_1925-01-01</th>\n",
       "      <th>year_1934-01-01</th>\n",
       "      <th>year_1937-01-01</th>\n",
       "      <th>year_1938-01-01</th>\n",
       "      <th>year_1939-01-01</th>\n",
       "      <th>year_1940-01-01</th>\n",
       "      <th>year_1941-01-01</th>\n",
       "      <th>...</th>\n",
       "      <th>countries_[81, 121, 102, 146]</th>\n",
       "      <th>countries_[81, 121]</th>\n",
       "      <th>countries_[81]</th>\n",
       "      <th>countries_[83, 102]</th>\n",
       "      <th>countries_[83, 242, 102]</th>\n",
       "      <th>countries_[83]</th>\n",
       "      <th>countries_[84]</th>\n",
       "      <th>countries_[85, 122, 121, 102]</th>\n",
       "      <th>countries_[93, 109]</th>\n",
       "      <th>countries_[]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  year_1895-01-01  year_1906-01-01  year_1925-01-01  \\\n",
       "0        0                0                0                0   \n",
       "3        3                0                0                0   \n",
       "4        4                0                0                0   \n",
       "5        5                0                0                0   \n",
       "6        6                0                0                0   \n",
       "\n",
       "   year_1934-01-01  year_1937-01-01  year_1938-01-01  year_1939-01-01  \\\n",
       "0                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "5                0                0                0                0   \n",
       "6                0                0                0                0   \n",
       "\n",
       "   year_1940-01-01  year_1941-01-01  ...  countries_[81, 121, 102, 146]  \\\n",
       "0                0                0  ...                              0   \n",
       "3                0                0  ...                              0   \n",
       "4                0                0  ...                              0   \n",
       "5                0                0  ...                              0   \n",
       "6                0                0  ...                              0   \n",
       "\n",
       "   countries_[81, 121]  countries_[81]  countries_[83, 102]  \\\n",
       "0                    0               0                    0   \n",
       "3                    0               0                    0   \n",
       "4                    0               0                    0   \n",
       "5                    0               0                    0   \n",
       "6                    0               0                    0   \n",
       "\n",
       "   countries_[83, 242, 102]  countries_[83]  countries_[84]  \\\n",
       "0                         0               0               0   \n",
       "3                         0               0               0   \n",
       "4                         0               0               0   \n",
       "5                         0               0               0   \n",
       "6                         0               0               0   \n",
       "\n",
       "   countries_[85, 122, 121, 102]  countries_[93, 109]  countries_[]  \n",
       "0                              0                    0             0  \n",
       "3                              0                    0             0  \n",
       "4                              0                    0             0  \n",
       "5                              0                    0             0  \n",
       "6                              0                    0             0  \n",
       "\n",
       "[5 rows x 1303 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим фичи айтемов\n",
    "items_df = movies.rename(columns={'id':'item_id'})\n",
    "\n",
    "item_cat_feats = ['year', 'genres', 'countries']\n",
    "\n",
    "items_ohe_df = items_df.item_id\n",
    "\n",
    "for feat in item_cat_feats:\n",
    "  ohe_feat_df = pd.get_dummies(items_df[feat], prefix=feat)\n",
    "  items_ohe_df = pd.concat([items_ohe_df, ohe_feat_df], axis=1) \n",
    "\n",
    "items_ohe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N users before: 182969\n",
      "N items before: 5237\n",
      "\n",
      "N users after: 66842\n",
      "N items after: 4226\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Собираем матрицу взаимодействий\n",
    "В датасете взаимодействий есть непопулярные фильмы и малоактивные пользователи. Кроме того, в таблице взаимодействий есть события с низким качеством взаимодействия - когда юзер начал смотреть фильм, но вскоре после начала просмотра выключил. \n",
    "\n",
    "Отфильтруем такие события*, малоактивных юзеров и непопулярные фильмы.\n",
    "\n",
    "_Можете не фильтровать такие события, тогда у вас будет больше негативных примеров._\n",
    "'''\n",
    "interactions_df = logs_train.groupby(['user_id', 'movie_id'], as_index=False).agg({'datetime':'max', 'duration':'sum'}).rename(columns={'movie_id':'item_id', 'duration':'total_dur', 'datetime':'last_watch_dt'})\n",
    "\n",
    "print(f\"N users before: {interactions_df.user_id.nunique()}\")\n",
    "print(f\"N items before: {interactions_df.item_id.nunique()}\\n\")\n",
    "\n",
    "# отфильтруем все события взаимодействий, в которых пользователь посмотрел\n",
    "# фильм менее чем на 35 процентов\n",
    "# замена на 10 минут\n",
    "interactions_df = interactions_df[interactions_df.total_dur > 600]\n",
    "\n",
    "# соберем всех пользователей, которые посмотрели \n",
    "# больше 10 фильмов (можете выбрать другой порог)\n",
    "valid_users = []\n",
    "\n",
    "c = Counter(interactions_df.user_id)\n",
    "for user_id, entries in c.most_common():\n",
    "  if entries > 2:\n",
    "    valid_users.append(user_id)\n",
    "\n",
    "# и соберем все фильмы, которые посмотрели больше 10 пользователей\n",
    "valid_items = []\n",
    "\n",
    "c = Counter(interactions_df.item_id)\n",
    "for item_id, entries in c.most_common():\n",
    "  if entries > 5:\n",
    "    valid_items.append(item_id)\n",
    "\n",
    "# отбросим непопулярные фильмы и неактивных юзеров\n",
    "interactions_df = interactions_df[interactions_df.user_id.isin(valid_users)]\n",
    "interactions_df = interactions_df[interactions_df.item_id.isin(valid_items)]\n",
    "\n",
    "print(f\"N users after: {interactions_df.user_id.nunique()}\")\n",
    "print(f\"N items after: {interactions_df.item_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66842\n",
      "3849\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "После фильтрации может получиться так, что некоторые айтемы/юзеры есть в датасете взаимодействий, но при этом они отсутствуют в датасетах айтемов/юзеров или наоборот. \n",
    "Поэтому найдем id айтемов и id юзеров, которые есть во всех датасетах и оставим только их.\n",
    "'''\n",
    "common_users = set(interactions_df.user_id.unique()).intersection(set(users_ohe_df.user_id.unique()))\n",
    "common_items = set(interactions_df.item_id.unique()).intersection(set(items_ohe_df.item_id.unique()))\n",
    "\n",
    "print(len(common_users))\n",
    "print(len(common_items))\n",
    "\n",
    "interactions_df = interactions_df[interactions_df.item_id.isin(common_items)]\n",
    "interactions_df = interactions_df[interactions_df.user_id.isin(common_users)]\n",
    "\n",
    "items_ohe_df = items_ohe_df[items_ohe_df.item_id.isin(common_items)]\n",
    "users_ohe_df = users_ohe_df[users_ohe_df.user_id.isin(common_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем взаимодействия в матрицу user*item так, чтобы в строках этой матрицы были user_id, в столбцах - item_id, а на пересечениях строк и столбцов - единица, если пользователь взаимодействовал с айтемом и ноль, если нет.\n",
    "\n",
    "Такую матрицу удобно собирать в numpy array, однако нужно помнить, что numpy array индексируется порядковыми индексами, а нам же удобнее использовать item_id и user_id.\n",
    "\n",
    "Создадим некие внутренние индексы для user_id и item_id - uid и iid. Для этого просто соберем все user_id и item_id и пронумеруем их по порядку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>last_watch_dt</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>uid</th>\n",
       "      <th>iid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-05-23 21:20:53.423633+03:00</td>\n",
       "      <td>4348</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>2023-05-28 06:38:40.736898+03:00</td>\n",
       "      <td>31497</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>2023-05-31 20:35:46.239771+03:00</td>\n",
       "      <td>92347</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>2023-04-18 11:59:47.414031+03:00</td>\n",
       "      <td>2286</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>2023-05-31 20:29:44.903470+03:00</td>\n",
       "      <td>16250</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id                    last_watch_dt  total_dur  uid  iid\n",
       "0        0       12 2023-05-23 21:20:53.423633+03:00       4348    0    4\n",
       "1        0       74 2023-05-28 06:38:40.736898+03:00      31497    0   29\n",
       "2        0      107 2023-05-31 20:35:46.239771+03:00      92347    0   43\n",
       "4        0      165 2023-04-18 11:59:47.414031+03:00       2286    0   81\n",
       "5        0      282 2023-05-31 20:29:44.903470+03:00      16250    0  142"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[\"uid\"] = interactions_df[\"user_id\"].astype(\"category\")\n",
    "interactions_df[\"uid\"] = interactions_df[\"uid\"].cat.codes\n",
    "\n",
    "interactions_df[\"iid\"] = interactions_df[\"item_id\"].astype(\"category\")\n",
    "interactions_df[\"iid\"] = interactions_df[\"iid\"].cat.codes\n",
    "\n",
    "print(sorted(interactions_df.iid.unique())[:5])\n",
    "print(sorted(interactions_df.uid.unique())[:5])\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец соберем и отнормируем матрицу взаимодействий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_vec = np.zeros((interactions_df.uid.nunique(), \n",
    "                             interactions_df.iid.nunique())) \n",
    "\n",
    "for user_id, item_id in zip(interactions_df.uid, interactions_df.iid):\n",
    "    interactions_vec[user_id, item_id] += 1\n",
    "\n",
    "\n",
    "res = interactions_vec.sum(axis=1)\n",
    "for i in range(len(interactions_vec)):\n",
    "    interactions_vec[i] /= res[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "3849\n",
      "66710\n",
      "66842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(interactions_df.item_id.nunique())\n",
    "print(items_ohe_df.item_id.nunique())\n",
    "print(interactions_df.user_id.nunique())\n",
    "print(users_ohe_df.user_id.nunique())\n",
    "\n",
    "set(items_ohe_df.item_id.unique()) - set(interactions_df.item_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы можно было удобно превратить iid/uid в item_id/user_id и наоборот соберем словари \n",
    "\n",
    "{iid: item_id}, {uid: user_id} и {item_id: iid}, {user_id: uid}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_to_item_id = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"iid\").to_dict()[\"item_id\"]\n",
    "item_id_to_iid = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"item_id\").to_dict()[\"iid\"]\n",
    "\n",
    "uid_to_user_id = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"uid\").to_dict()[\"user_id\"]\n",
    "user_id_to_uid = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"user_id\").to_dict()[\"uid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И проиндексируем датасеты users_ohe_df и items_ohe_df по внутренним айди:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ohe_df[\"iid\"] = items_ohe_df[\"item_id\"].apply(lambda x: item_id_to_iid[x])\n",
    "items_ohe_df = items_ohe_df.set_index(\"iid\")\n",
    "\n",
    "users_ohe_df[\"uid\"] = users_ohe_df[\"user_id\"].apply(lambda x: user_id_to_uid.get(x, 'unknown'))\n",
    "users_ohe_df = users_ohe_df.set_index(\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM starter's pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем вектор юзера (anchor) и векторы двух айтемов - \"хорошего\" и \"плохого\" (positive и negative). Хороший айтем - это тот, который пользователь уже посмотрел, а в качестве плохого возьмем любой случайный айтем из датасета. Затем посчитаем расстояния:\n",
    "1. между вектором юзера и вектором \"хорошего\" айтема\n",
    "2. между вектором юзера и вектором \"плохого\" айтема\n",
    "\n",
    "_Значением функции потерь будет разность между первым и вторым расстоянием._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, n_dims=128, alpha=0.4):\n",
    "    # будем ожидать, что на вход функции прилетит три сконкатенированных \n",
    "    # вектора - вектор юзера и два вектора айтема\n",
    "    anchor = y_pred[:, 0:n_dims]\n",
    "    positive = y_pred[:, n_dims:n_dims*2]\n",
    "    negative = y_pred[:, n_dims*2:n_dims*3]\n",
    "\n",
    "    # считаем расстояния от вектора юзера до вектора хорошего айтема\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "    # и до плохого\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "\n",
    "    # считаем лосс\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = K.maximum(basic_loss, 0.0) # возвращаем ноль, если лосс отрицательный\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(items, users, interactions, batch_size=1024):\n",
    "    while True:\n",
    "        uid_meta = []\n",
    "        uid_interaction = []\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for _ in range(batch_size):\n",
    "            # берем рандомный uid\n",
    "            uid_i = randint(0, interactions.shape[0]-1)\n",
    "            # id хорошего айтема\n",
    "            pos_i = np.random.choice(range(interactions.shape[1]), p=interactions[uid_i])\n",
    "            # id плохого айтема\n",
    "            neg_i = np.random.choice(range(interactions.shape[1]))\n",
    "            # фичи юзера\n",
    "            uid_meta.append(users.iloc[uid_i])\n",
    "            # вектор айтемов, с которыми юзер взаимодействовал\n",
    "            uid_interaction.append(interactions_vec[uid_i])\n",
    "            # фичи хорошего айтема\n",
    "            pos.append(items.iloc[pos_i])\n",
    "            # фичи плохого айтема\n",
    "            neg.append(items.iloc[neg_i])\n",
    "            \n",
    "        yield [np.array(uid_meta), np.array(uid_interaction), np.array(pos), np.array(neg)], [np.array(uid_meta), np.array(uid_interaction)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вектор фичей юзера: (1024, 8)\n",
      "вектор взаимодействий юзера с айтемами: (1024, 3849)\n",
      "вектор 'хорошего' айтема: (1024, 1302)\n",
      "вектор 'плохого' айтема: (1024, 1302)\n",
      "\n",
      "вектор фичей юзера: (1024, 8)\n",
      "вектор взаимодействий юзера с айтемами: (1024, 3849)\n"
     ]
    }
   ],
   "source": [
    "# инициализируем генератор\n",
    "gen = generator(items=items_ohe_df.drop([\"item_id\"], axis=1), \n",
    "                users=users_ohe_df.drop([\"user_id\"], axis=1), \n",
    "                interactions=interactions_vec)\n",
    "\n",
    "ret = next(gen)\n",
    "\n",
    "\n",
    "print(f\"вектор фичей юзера: {ret[0][0].shape}\")\n",
    "print(f\"вектор взаимодействий юзера с айтемами: {ret[0][1].shape}\")\n",
    "print(f\"вектор 'хорошего' айтема: {ret[0][2].shape}\")\n",
    "print(f\"вектор 'плохого' айтема: {ret[0][3].shape}\")\n",
    "print()\n",
    "print(f\"вектор фичей юзера: {ret[1][0].shape}\")\n",
    "print(f\"вектор взаимодействий юзера с айтемами: {ret[1][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы обучить модель используя триплет лосс, нам нужно получить три вектора - вектор юзера, вектор \"хорошего\" айтема и вектор \"плохого\" айтема. Для этого нам нужно две модели (\"хороший\" и \"плохой\" айтем будут семплироваться одной и той же моделью).  \n",
    "\n",
    "Модель юзера будет иметь два входа:\n",
    "- вход для фичей юзера (фичи из users_ohe_df)\n",
    "- вход для вектора айтемов, которые посмотрел юзер (строка interactions_vec, которая соответствует uid конкретного юзера)\n",
    "\n",
    "Выход модели юзера будет размерностью N_FACTORS.\n",
    "\n",
    "У модели айтема будет один вход для фичей айтема (из items_ohe_df) и один выход также размерностью N_FACTORS.\n",
    "\n",
    "Общая архитектура будет вот такой: \n",
    "- есть модель юзера и модель айтема\n",
    "- обе модели семплируют юзер и айтем-фичи во внутреннее пространство размерностью N_FACTORS\n",
    "- модель айтема семплирует два айтема - \"хороший\" и \"плохой\"\n",
    "- в итоге получается три вектора размерностью N_FACTORS (вектор юзера, вектор \"хорошего\" айтема и вектор \"плохого\" айтема)\n",
    "- затем полученные векторы конкатенируются, по ним считается triplet loss\n",
    "- profit\n",
    "\n",
    "Для того, чтобы собрать модель, помимо размерности внутреннего пространства, нам нужно знать еще размерность вектора юзера и вектора айтема. Зададим их сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_FACTORS: 128\n",
      "ITEM_MODEL_SHAPE: (1302,)\n",
      "USER_META_MODEL_SHAPE: (8,)\n",
      "USER_INTERACTION_MODEL_SHAPE: (3849,)\n"
     ]
    }
   ],
   "source": [
    "N_FACTORS = 128\n",
    "\n",
    "# в датасетах есть столбец user_id/item_id, помним, что он не является фичей для обучения!\n",
    "ITEM_MODEL_SHAPE = (items_ohe_df.drop([\"item_id\"], axis=1).shape[1], ) \n",
    "USER_META_MODEL_SHAPE = (users_ohe_df.drop([\"user_id\"], axis=1).shape[1], )\n",
    "\n",
    "USER_INTERACTION_MODEL_SHAPE = (interactions_vec.shape[1], )\n",
    "\n",
    "print(f\"N_FACTORS: {N_FACTORS}\")\n",
    "print(f\"ITEM_MODEL_SHAPE: {ITEM_MODEL_SHAPE}\")\n",
    "print(f\"USER_META_MODEL_SHAPE: {USER_META_MODEL_SHAPE}\")\n",
    "print(f\"USER_INTERACTION_MODEL_SHAPE: {USER_INTERACTION_MODEL_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_model(n_factors=N_FACTORS):\n",
    "    # входной слой\n",
    "    inp = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "    \n",
    "    # полносвязный слой\n",
    "    layer_1 = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                               kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                               activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp)\n",
    "\n",
    "    # делаем residual connection - складываем два слоя, \n",
    "    # чтобы градиенты не затухали во время обучения\n",
    "    layer_2 = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(layer_1)\n",
    "    \n",
    "    add = keras.layers.Add()([layer_1, layer_2])\n",
    "    \n",
    "    # выходной слой\n",
    "    out = keras.layers.Dense(N_FACTORS, activation='linear', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(add)\n",
    "    \n",
    "    return keras.models.Model(inp, out)\n",
    "\n",
    "\n",
    "def user_model(n_factors=N_FACTORS):\n",
    "    # входной слой для вектора фичей юзера (из users_ohe_df)\n",
    "    inp_meta = keras.layers.Input(shape=USER_META_MODEL_SHAPE)\n",
    "    # входной слой для вектора просмотров (из iteractions_vec)\n",
    "    inp_interaction = keras.layers.Input(shape=USER_INTERACTION_MODEL_SHAPE)\n",
    "\n",
    "    # полносвязный слой\n",
    "    layer_1_meta = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp_meta)\n",
    "\n",
    "    layer_1_interaction = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp_interaction)\n",
    "\n",
    "    # делаем residual connection - складываем два слоя,\n",
    "    # чтобы градиенты не затухали во время обучения\n",
    "    layer_2_meta = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(layer_1_meta)\n",
    "    \n",
    "\n",
    "    add = keras.layers.Add()([layer_1_meta, layer_2_meta])\n",
    "    \n",
    "    # конкатенируем вектор фичей с вектором просмотров\n",
    "    concat_meta_interaction = keras.layers.Concatenate()([add, layer_1_interaction])\n",
    "    \n",
    "    # выходной слой\n",
    "    out = keras.layers.Dense(N_FACTORS, activation='linear', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(concat_meta_interaction)\n",
    "    \n",
    "    return keras.models.Model([inp_meta, inp_interaction], out)\n",
    "\n",
    "# инициализируем модели юзера и айтема\n",
    "i2v = item_model()\n",
    "u2v = user_model()\n",
    "\n",
    "# вход для вектора фичей юзера (из users_ohe_df)\n",
    "ancor_meta_in = keras.layers.Input(shape=USER_META_MODEL_SHAPE)\n",
    "# вход для вектора просмотра юзера (из interactions_vec)\n",
    "ancor_interaction_in = keras.layers.Input(shape=USER_INTERACTION_MODEL_SHAPE)\n",
    "\n",
    "# вход для вектора \"хорошего\" айтема\n",
    "pos_in = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "# вход для вектора \"плохого\" айтема\n",
    "neg_in = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "\n",
    "# получаем вектор юзера\n",
    "ancor = u2v([ancor_meta_in, ancor_interaction_in])\n",
    "# получаем вектор \"хорошего\" айтема\n",
    "pos = i2v(pos_in)\n",
    "# получаем вектор \"плохого\" айтема\n",
    "neg = i2v(neg_in)\n",
    "\n",
    "# конкатенируем полученные векторы\n",
    "res = keras.layers.Concatenate(name=\"concat_ancor_pos_neg\")([ancor, pos, neg])\n",
    "\n",
    "# собираем модель\n",
    "model = keras.models.Model([ancor_meta_in, ancor_interaction_in, pos_in, neg_in], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'recsys_resnet_linear'\n",
    "\n",
    "# логируем процесс обучения в тензорборд\n",
    "t_board = keras.callbacks.TensorBoard(log_dir=f'runs/{model_name}')\n",
    "\n",
    "# уменьшаем learning_rate, если лосс долго не уменьшается (в течение двух эпох)\n",
    "decay = keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, factor=0.8, verbose=1)\n",
    "\n",
    "# сохраняем модель после каждой эпохи, если лосс уменьшился\n",
    "check = keras.callbacks.ModelCheckpoint(filepath=model_name + '/epoch{epoch}-{loss:.2f}.h5', monitor=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# компилируем модель, используем оптимайзер Adam и triplet loss\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss=triplet_loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 1302)]               0         []                            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  166656    ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 128)                  16384     ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 128)                  0         ['dense_7[0][0]',             \n",
      "                                                                     'dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 128)                  16384     ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 199424 (779.00 KB)\n",
      "Trainable params: 199424 (779.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# модель айтема\n",
    "item_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 128)                  1024      ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 128)                  16384     ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 3849)]               0         []                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 128)                  0         ['dense_10[0][0]',            \n",
      "                                                                     'dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 128)                  492672    ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 256)                  0         ['add_3[0][0]',               \n",
      " )                                                                   'dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 128)                  32768     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 542848 (2.07 MB)\n",
      "Trainable params: 542848 (2.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# модель юзера\n",
    "user_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 3849)]               0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 1302)]               0         []                            \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 1302)]               0         []                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, 128)                  542848    ['input_4[0][0]',             \n",
      "                                                                     'input_5[0][0]']             \n",
      "                                                                                                  \n",
      " model (Functional)          (None, 128)                  199424    ['input_6[0][0]',             \n",
      "                                                                     'input_7[0][0]']             \n",
      "                                                                                                  \n",
      " concat_ancor_pos_neg (Conc  (None, 384)                  0         ['model_1[0][0]',             \n",
      " atenate)                                                            'model[0][0]',               \n",
      "                                                                     'model[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 742272 (2.83 MB)\n",
      "Trainable params: 742272 (2.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# общая модель\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 11s 84ms/step - loss: 0.2969 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.2288 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.1879 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.1634 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 0.1492 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 0.1367 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.1345 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.1240 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.1197 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.1205 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.1074 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.1111 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 6s 63ms/step - loss: 0.1025 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.1053 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.1022 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 7s 69ms/step - loss: 0.0968 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.0995 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.0946 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.0950 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.0901 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.0939 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.0858 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.0889 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.0818 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 70ms/step - loss: 0.0848 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0880\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.0880 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.0847 - lr: 8.0000e-04\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.0798 - lr: 8.0000e-04\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.0791 - lr: 8.0000e-04\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 6s 61ms/step - loss: 0.0745 - lr: 8.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21b89942700>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# начинаем обучение, не забывая дропнуть столбцы item_id и user_id \n",
    "# из датафреймов при инициализации генератора.\n",
    "\n",
    "# batch_size можно (и лучше) поставить побольше, если вы не органичены в ресурсах\n",
    "\n",
    "model.fit(generator(items=items_ohe_df.drop([\"item_id\"], axis=1), \n",
    "                    users=users_ohe_df.drop([\"user_id\"], axis=1), \n",
    "                    interactions=interactions_vec,\n",
    "                    batch_size=64), \n",
    "          steps_per_epoch=100, \n",
    "          epochs=30, \n",
    "          initial_epoch=0,\n",
    "          callbacks=[decay, t_board, check]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (пример на слуяайном)\n",
    "\n",
    "Отлично! Мы подготовили данные, собрали модель по архитектуре DSSM и обучили ее.\n",
    "Теперь возьмем случайного юзера и случайный айтем. Как понять, насколько этот айтем релевантен юзеру?\n",
    "\n",
    "Нужно:\n",
    "- получить вектор айтема;\n",
    "- получить вектор юзера;\n",
    "- посчитать расстояние между ними.\n",
    "\n",
    "Это расстояние и есть мера релевантности.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.6206167]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# берем рандомного юзера\n",
    "rand_uid = np.random.choice(list(users_ohe_df.index))\n",
    "\n",
    "# получаем фичи юзера и вектор его просмотров айтемов\n",
    "user_meta_feats = users_ohe_df.drop([\"user_id\"], axis=1).iloc[rand_uid.astype(int)]\n",
    "user_interaction_vec = interactions_vec[rand_uid.astype(int)]\n",
    "\n",
    "# берем рандомный айтем\n",
    "rand_iid = np.random.choice(list(items_ohe_df.index))\n",
    "# получаем фичи айтема\n",
    "item_feats = items_ohe_df.drop([\"item_id\"], axis=1).iloc[rand_iid]\n",
    "\n",
    "# получаем вектор юзера\n",
    "user_vec = u2v.predict([np.array(user_meta_feats).reshape(1, -1), \n",
    "                        np.array(user_interaction_vec).reshape(1, -1)])\n",
    "\n",
    "# и вектор айтема\n",
    "item_vec = i2v.predict(np.array(item_feats).reshape(1, -1))\n",
    "\n",
    "# считаем расстояние между вектором юзера и вектором айтема\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ED\n",
    "\n",
    "ED(user_vec, item_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# получаем фичи всех айтемов\n",
    "items_feats = items_ohe_df.drop([\"item_id\"], axis=1).to_numpy()\n",
    "# получаем векторы всех айтемов\n",
    "items_vecs = i2v.predict(items_feats)\n",
    "\n",
    "# считаем расстояния\n",
    "dists = ED(user_vec, items_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1788,  823, 1300, 2774, 3597,  116, 3383, 2679, 2365, 3632,  967,\n",
       "        805, 1926,   84, 2233, 1676,  237, 3156, 3546,  592], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_iids = np.argsort(dists, axis=1)[0][:20]\n",
    "top5_iids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось конвертировать внутренние iid в ~~id здорового человека~~ item_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_item_ids = [iid_to_item_id[iid] for iid in top5_iids]\n",
    "df_pred_u = pd.DataFrame({'item_id': top5_item_ids})\n",
    "df_pred_u['rank'] = df_pred_u.index+1\n",
    "df_pred_u['user_id'] = rand_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172                                 Молодожены\n",
       "238                            Разум и чувства\n",
       "486                                   Красотка\n",
       "1191                                Век Адалин\n",
       "1597                                      Шери\n",
       "1628                                  Близость\n",
       "1907                             Мемуары гейши\n",
       "2567                                Выбор Софи\n",
       "3277    Красавчик Алфи, или Чего хотят мужчины\n",
       "3500                                      Эмма\n",
       "3771                           Умереть молодым\n",
       "4328                         Ромео и Джульетта\n",
       "4594                                Секретарша\n",
       "5228                                  Мажестик\n",
       "5416                               Это все она\n",
       "6143                             Перед закатом\n",
       "6587                            Сладкий ноябрь\n",
       "6863                          Интимный словарь\n",
       "6944                              Ноттинг Хилл\n",
       "7022                             Запах женщины\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_titles = items_df.loc[items_df.item_id.isin(top5_item_ids)].name\n",
    "recommended_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference для всей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s 2ms/step\n",
      "2085/2085 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances as ED\n",
    "\n",
    "# Получаем фичи всех айтемов и векторы всех айтемов заранее\n",
    "items_feats = items_ohe_df.drop([\"item_id\"], axis=1).to_numpy()\n",
    "items_vecs = i2v.predict(items_feats)\n",
    "\n",
    "# Получаем фичи юзеров и векторы их просмотров айтемов\n",
    "user_meta_feats = users_ohe_df.drop('unknown', axis=0).drop([\"user_id\"], axis=1).to_numpy()\n",
    "user_interaction_vecs = interactions_vec  # Нет необходимости использовать to_numpy()\n",
    "\n",
    "# Получаем векторы юзеров\n",
    "user_vecs = u2v.predict([user_meta_feats, user_interaction_vecs])\n",
    "\n",
    "# Считаем расстояния между векторами юзеров и векторами всех айтемов\n",
    "dists = ED(user_vecs, items_vecs)\n",
    "\n",
    "# Находим ТОП-20 айтемов для каждого пользователя\n",
    "top20_iids = np.argsort(dists, axis=1)[:, :20]\n",
    "top20_item_ids = np.vectorize(iid_to_item_id.get)(top20_iids)  # Применяем маппинг iid в item_id\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "df_pred = pd.DataFrame({\n",
    "    'user_id': np.repeat(users_ohe_df.index, 20)[:len(top20_item_ids.flatten())],\n",
    "    'item_id': top20_item_ids.flatten(),\n",
    "    'rank': np.tile(np.arange(1, 21), len(users_ohe_df))[:len(top20_item_ids.flatten())]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расчет метрики МАР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_self(df_true, df_pred, k=20):\n",
    "    # Функция расчета МАР\n",
    "    ap_sum = 0\n",
    "    for i in df_pred['user_id'].unique().tolist():\n",
    "        t=0\n",
    "        w_sum=0\n",
    "        items_true = df_true[df_true['user_id']==i]['item_id'].unique().tolist()\n",
    "        items_pred = df_pred[df_pred['user_id']==i]['item_id'].unique().tolist()\n",
    "        for x in range(k):\n",
    "            if items_pred[x] in items_true:\n",
    "                t +=1\n",
    "                w_sum = w_sum + t/(x+1)\n",
    "        ap_sum = ap_sum+w_sum/k\n",
    "    map = ap_sum/len(df_pred)\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(df_true, df_pred, k=20, target_col='rank'):\n",
    "\n",
    "    # Объединение данных по 'user_id' и 'item_id'\n",
    "\n",
    "    df = df_true.set_index(['user_id', 'item_id']).join(df_pred.set_index(['user_id', 'item_id']))\n",
    "\n",
    "    # Отбор пользователей, которые есть и в обучающей, и в тестовой выборке\n",
    "    common_users = set(df_true['user_id']).intersection(set(df_pred['user_id']))\n",
    "    df = df[df.index.get_level_values('user_id').isin(common_users)]\n",
    "\n",
    "    # Сортировка данных по 'user_id' и предсказаниям\n",
    "    df = df.sort_values(by=['user_id', target_col], ascending=[True, False])\n",
    "\n",
    "    # Инициализация переменных для расчета MAP\n",
    "    total_precision = 0.0\n",
    "    users_count = 0\n",
    "\n",
    "    # Расчет MAP для каждого пользователя\n",
    "    for user_id, user_df in df.groupby(level='user_id'):\n",
    "        user_precision = 0.0\n",
    "        relevant_items = 0\n",
    "\n",
    "        for i, (_, row) in enumerate(user_df.head(k).iterrows(), 1):\n",
    "            # Рассчитываем W = P / (N * R)\n",
    "            precision = row['interaction'] / i\n",
    "            user_precision += precision\n",
    "            relevant_items += row['interaction']\n",
    "\n",
    "        if relevant_items > 0:\n",
    "            user_precision /= relevant_items  # Усреднение для пользователя\n",
    "            total_precision += user_precision\n",
    "            users_count += 1\n",
    "\n",
    "\n",
    "    # Расчет среднего значения MAP по всем пользователям\n",
    "    if users_count > 0:\n",
    "        mean_ap = total_precision / users_count\n",
    "        return mean_ap\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5658399578908294"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(logs_test, df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vazhitenev\\RecSys\\YOUR SECOND RECSYS.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m map_v2(logs_test, df_pred, \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\vazhitenev\\RecSys\\YOUR SECOND RECSYS.ipynb Cell 55\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m pred_items \u001b[39m=\u001b[39m df_pred\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrank\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mitem_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlist\u001b[39m)\u001b[39m.\u001b[39mreset_index(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mitem_list_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m merget_array \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(test_items, pred_items, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mapk(merget_array[\u001b[39m'\u001b[39;49m\u001b[39mitem_list_test\u001b[39;49m\u001b[39m'\u001b[39;49m], merget_array[\u001b[39m'\u001b[39;49m\u001b[39mitem_list_pred\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m20\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\vazhitenev\\RecSys\\YOUR SECOND RECSYS.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmapk\u001b[39m(actual, predicted, k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([apk(a, p, k) \u001b[39mfor\u001b[39;00m a, p \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(actual, predicted)])\n",
      "\u001b[1;32mc:\\Users\\vazhitenev\\RecSys\\YOUR SECOND RECSYS.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmapk\u001b[39m(actual, predicted, k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean([apk(a, p, k) \u001b[39mfor\u001b[39;00m a, p \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(actual, predicted)])\n",
      "\u001b[1;32mc:\\Users\\vazhitenev\\RecSys\\YOUR SECOND RECSYS.ipynb Cell 55\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m actual:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(predicted)\u001b[39m>\u001b[39mk:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     predicted \u001b[39m=\u001b[39m predicted[:k]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vazhitenev/RecSys/YOUR%20SECOND%20RECSYS.ipynb#Y120sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "map_v2(logs_test, df_pred, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_self(logs_test, df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6560723602456451"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "mean_average_precision(logs_test, bl_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map_self(logs_test, bl_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame.copy(logs_test)\n",
    "df_pred = pd.DataFrame.copy(bl_pred)   \n",
    "df = df_true.set_index(['user_id', 'item_id']).join(df_pred.set_index(['user_id', 'item_id']))\n",
    "\n",
    "# Отбор пользователей, которые есть и в обучающей, и в тестовой выборке\n",
    "common_users = set(df_true['user_id']).intersection(set(df_pred['user_id']))\n",
    "df = df[df.index.get_level_values('user_id').isin(common_users)]\n",
    "# Сортировка данных по 'user_id' и предсказаниям\n",
    "df = df.sort_values(by=['user_id', 'rank'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk (actual, predicted, k=10):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def map_v2(df_true, df_pred, k=10):\n",
    "    test_items = df_true.groupby('user_id')['item_id'].apply(list).reset_index(name='item_list_test')\n",
    "    pred_items = df_pred.sort_values(by='rank').groupby('user_id')['item_id'].apply(list).reset_index(name='item_list_pred')\n",
    "    merget_array = pd.merge(test_items, pred_items, on='user_id', how='left')\n",
    "    return mapk(merget_array['item_list_test'], merget_array['item_list_pred'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame.copy(logs_test)\n",
    "test_items = df_true.groupby('user_id')['item_id'].apply(list).reset_index(name='item_list_test')\n",
    "pred_items = df_pred.sort_values(by='rank').groupby('user_id')['item_id'].apply(list).reset_index(name='item_list_pred')\n",
    "merget_array = pd.merge(test_items, pred_items, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006090113969266866"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([apk(a, p, 20) for a, p in zip(merget_array['item_list_test'], merget_array['item_list_pred'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, p in zip(merget_array['item_list_test'], merget_array['item_list_pred']):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5822,\n",
       " 3118,\n",
       " 6914,\n",
       " 4207,\n",
       " 6013,\n",
       " 5059,\n",
       " 432,\n",
       " 2337,\n",
       " 5893,\n",
       " 1032,\n",
       " 3402,\n",
       " 5141,\n",
       " 2490,\n",
       " 2869,\n",
       " 4341,\n",
       " 4483,\n",
       " 3676,\n",
       " 892,\n",
       " 282,\n",
       " 5928]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
