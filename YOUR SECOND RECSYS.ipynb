{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from random import randint, random\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_movies = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/movies.csv\"\n",
    "p_countries = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/countries.csv\"\n",
    "p_genres = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/genres.csv\"\n",
    "p_staff = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/staff.csv\"\n",
    "p_logs = \"C:/Users/vazhitenev/PycharmProjects/Contest2023/oneproj/train/logs.csv\"\n",
    "'''\n",
    "p_movies = \"C:/Users/Vergas\\Contest2023/train/movies.csv\"\n",
    "p_countries = \"C:/Users/Vergas/Contest2023/train/countries.csv\"\n",
    "p_genres = \"C:/Users/Vergas/Contest2023/train/genres.csv\"\n",
    "p_staff = \"C:/Users/Vergas/Contest2023/train/staff.csv\"\n",
    "p_logs = \"C:/Users/Vergas/Contest2023/train/logs.csv\"\n",
    "'''\n",
    "movies = pd.read_csv(p_movies)\n",
    "countries = pd.read_csv(p_countries)\n",
    "genres = pd.read_csv(p_genres)\n",
    "staff = pd.read_csv(p_staff)\n",
    "logs = pd.read_csv(p_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#удаляем из списка фильмо те, которые не были опубликованы\n",
    "movies = movies[~movies['date_publication'].isnull()]\n",
    "# переводим тип float to intager\n",
    "logs['movie_id'] = logs['movie_id'].astype(int)\n",
    "logs['duration'] = logs['duration'].astype(int)\n",
    "logs['datetime'] = pd.to_datetime(logs['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nprint(f\"min date in filtered interactions: {MAX_DATE}\")\\nprint(f\"max date in filtered interactions:: {MIN_DATE}\")\\nprint(f\"test max date to split:: {TEST_MAX_DATE}\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set dates params for filter\n",
    "MAX_DATE = logs['datetime'].max()\n",
    "MIN_DATE = logs['datetime'].min()\n",
    "TEST_INTERVAL_DAYS = 14\n",
    "TEST_MAX_DATE = MAX_DATE - dt.timedelta(days = TEST_INTERVAL_DAYS)\n",
    "''' \n",
    "print(f\"min date in filtered interactions: {MAX_DATE}\")\n",
    "print(f\"max date in filtered interactions:: {MIN_DATE}\")\n",
    "print(f\"test max date to split:: {TEST_MAX_DATE}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_test = logs.loc[logs['datetime'] >= TEST_MAX_DATE]\n",
    "logs_test = logs_test[[\"user_id\", \"movie_id\"]].drop_duplicates().rename(columns={'movie_id':'item_id'})\n",
    "logs_train = logs.loc[logs['datetime'] < TEST_MAX_DATE]\n",
    "logs_test = logs_test[logs_test['user_id'].isin(logs_train['user_id'].unique().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = logs_test.groupby('user_id')['item_id'].unique().apply(list).reset_index(name='item_list_test')\n",
    "train_items= logs_train.groupby('user_id')['movie_id'].unique().apply(list).reset_index(name='item_list_train')\n",
    "common_users = pd.merge(test_items, train_items, on='user_id', how='inner')\n",
    "common_users['item_list_check'] = common_users.apply(lambda row: list(set(row['item_list_test']) - set(row['item_list_train'])), axis=1)\n",
    "logs_test_norm = common_users.drop(['item_list_test', 'item_list_train'], axis= 1).explode('item_list_check')\n",
    "logs_test_norm = logs_test_norm[~logs_test_norm['item_list_check'].isna()].rename(columns={'item_list_check':'item_id'})\n",
    "logs_test_norm['interaction'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_movie = list(logs_train['movie_id'].value_counts().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_u(uid, l_test, pop_movie):\n",
    "    watched = list(l_test[l_test['user_id']==uid].movie_id)\n",
    "    pop_movie = [x for x in pop_movie if x not in watched][:20]\n",
    "    rank_df = pd.DataFrame({\n",
    "        'user_id': np.repeat(uid, 20),\n",
    "        'item_id': pop_movie,\n",
    "        'rank': np.arange(1, 21)\n",
    "    })\n",
    "    return rank_df\n",
    "\n",
    "\n",
    "bl_pred = pd.concat([pd.DataFrame(top_u(x, logs_train, pop_movie)) for x in logs_test['user_id'].unique()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_quantile_(0.0, 45743.0]</th>\n",
       "      <th>movie_quantile_(45743.0, 91485.0]</th>\n",
       "      <th>movie_quantile_(91485.0, 137227.0]</th>\n",
       "      <th>movie_quantile_(137227.0, 182969.0]</th>\n",
       "      <th>duration_quantile_(0.0, 45743.0]</th>\n",
       "      <th>duration_quantile_(45743.0, 91485.0]</th>\n",
       "      <th>duration_quantile_(91485.0, 137227.0]</th>\n",
       "      <th>duration_quantile_(137227.0, 182969.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_quantile_(0.0, 45743.0]  movie_quantile_(45743.0, 91485.0]  \\\n",
       "0        0                              0                                  0   \n",
       "1        1                              0                                  0   \n",
       "2        2                              0                                  0   \n",
       "3        3                              0                                  0   \n",
       "4        4                              0                                  0   \n",
       "\n",
       "   movie_quantile_(91485.0, 137227.0]  movie_quantile_(137227.0, 182969.0]  \\\n",
       "0                                   0                                    1   \n",
       "1                                   0                                    1   \n",
       "2                                   0                                    1   \n",
       "3                                   0                                    1   \n",
       "4                                   0                                    1   \n",
       "\n",
       "   duration_quantile_(0.0, 45743.0]  duration_quantile_(45743.0, 91485.0]  \\\n",
       "0                                 0                                     0   \n",
       "1                                 0                                     0   \n",
       "2                                 0                                     0   \n",
       "3                                 0                                     0   \n",
       "4                                 0                                     0   \n",
       "\n",
       "   duration_quantile_(91485.0, 137227.0]  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "\n",
       "   duration_quantile_(137227.0, 182969.0]  \n",
       "0                                       1  \n",
       "1                                       1  \n",
       "2                                       1  \n",
       "3                                       1  \n",
       "4                                       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим фичи пользователей\n",
    "users_df = logs_train.groupby('user_id', as_index=False).agg({'movie_id':'nunique', 'duration':'sum'}).rename(columns={'movie_id':'movie_count', 'duration':'sum_duration'})\n",
    "users_df['movie_quantile']   =pd.qcut(users_df['movie_count'].rank(method='first'), q=4, precision=0)\n",
    "users_df['duration_quantile']=pd.qcut(users_df['sum_duration'].rank(method='first'), q=4, precision=0)\n",
    "user_cat_feats = [\"movie_quantile\", \"duration_quantile\"]\n",
    "\n",
    "# из исходного датафрейма оставим только item_id - этот признак нам понадобится позже\n",
    "# для того, чтобы маппить айтемы из датафрейма с фильмами с айтемами \n",
    "# из датафрейма с взаимодействиями\n",
    "users_ohe_df = users_df.user_id\n",
    "for feat in user_cat_feats:\n",
    "  # получаем датафрейм с one-hot encoding для каждой категориальной фичи\n",
    "  ohe_feat_df = pd.get_dummies(users_df[feat], prefix=feat)\n",
    "  # конкатенируем ohe-hot датафрейм с датафреймом, \n",
    "  # который мы получили на предыдущем шаге\n",
    "  users_ohe_df = pd.concat([users_ohe_df, ohe_feat_df], axis=1)\n",
    "\n",
    "users_ohe_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>year_1895-01-01</th>\n",
       "      <th>year_1906-01-01</th>\n",
       "      <th>year_1925-01-01</th>\n",
       "      <th>year_1934-01-01</th>\n",
       "      <th>year_1937-01-01</th>\n",
       "      <th>year_1938-01-01</th>\n",
       "      <th>year_1939-01-01</th>\n",
       "      <th>year_1940-01-01</th>\n",
       "      <th>year_1941-01-01</th>\n",
       "      <th>...</th>\n",
       "      <th>countries_[81, 121, 102, 146]</th>\n",
       "      <th>countries_[81, 121]</th>\n",
       "      <th>countries_[81]</th>\n",
       "      <th>countries_[83, 102]</th>\n",
       "      <th>countries_[83, 242, 102]</th>\n",
       "      <th>countries_[83]</th>\n",
       "      <th>countries_[84]</th>\n",
       "      <th>countries_[85, 122, 121, 102]</th>\n",
       "      <th>countries_[93, 109]</th>\n",
       "      <th>countries_[]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  year_1895-01-01  year_1906-01-01  year_1925-01-01  \\\n",
       "0        0                0                0                0   \n",
       "3        3                0                0                0   \n",
       "4        4                0                0                0   \n",
       "5        5                0                0                0   \n",
       "6        6                0                0                0   \n",
       "\n",
       "   year_1934-01-01  year_1937-01-01  year_1938-01-01  year_1939-01-01  \\\n",
       "0                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "5                0                0                0                0   \n",
       "6                0                0                0                0   \n",
       "\n",
       "   year_1940-01-01  year_1941-01-01  ...  countries_[81, 121, 102, 146]  \\\n",
       "0                0                0  ...                              0   \n",
       "3                0                0  ...                              0   \n",
       "4                0                0  ...                              0   \n",
       "5                0                0  ...                              0   \n",
       "6                0                0  ...                              0   \n",
       "\n",
       "   countries_[81, 121]  countries_[81]  countries_[83, 102]  \\\n",
       "0                    0               0                    0   \n",
       "3                    0               0                    0   \n",
       "4                    0               0                    0   \n",
       "5                    0               0                    0   \n",
       "6                    0               0                    0   \n",
       "\n",
       "   countries_[83, 242, 102]  countries_[83]  countries_[84]  \\\n",
       "0                         0               0               0   \n",
       "3                         0               0               0   \n",
       "4                         0               0               0   \n",
       "5                         0               0               0   \n",
       "6                         0               0               0   \n",
       "\n",
       "   countries_[85, 122, 121, 102]  countries_[93, 109]  countries_[]  \n",
       "0                              0                    0             0  \n",
       "3                              0                    0             0  \n",
       "4                              0                    0             0  \n",
       "5                              0                    0             0  \n",
       "6                              0                    0             0  \n",
       "\n",
       "[5 rows x 1303 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим фичи айтемов #1\n",
    "items_df = pd.DataFrame.copy(movies).rename(columns={'id':'item_id'})\n",
    "\n",
    "item_cat_feats = ['year', 'genres', 'countries']\n",
    "\n",
    "items_ohe_df = items_df.item_id\n",
    "\n",
    "for feat in item_cat_feats:\n",
    "  ohe_feat_df = pd.get_dummies(items_df[feat], prefix=feat)\n",
    "  items_ohe_df = pd.concat([items_ohe_df, ohe_feat_df], axis=1) \n",
    "\n",
    "items_ohe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Готовим фичи айтемов #2\n",
    "def top_for_columns(column, topval, prefix):\n",
    "    total_count = {}\n",
    "    for row in column:\n",
    "        for value in row:\n",
    "            total_count[value] = total_count.get(value, 0) + 1\n",
    "    pc = pd.DataFrame(total_count, index=[0]).transpose().sort_values(by=0, ascending=False)\n",
    "    list_columns = pc[(pc[0]>topval)].index.values.tolist()\n",
    "    list_columns = [prefix+elt for elt in list_columns]\n",
    "    return list_columns\n",
    "\n",
    "items_df = pd.DataFrame.copy(movies).rename(columns={'id':'item_id'})\n",
    "items_df['staff_list'] = items_df.apply(lambda row: re.split(', ', row['staff'].strip('[]')), axis=1)\n",
    "items_df['genres_list'] = items_df.apply(lambda row: re.split(', ', row['genres'].strip('[]')), axis=1)\n",
    "items_df['countries_list'] = items_df.apply(lambda row: re.split(', ', row['countries'].strip('[]')), axis=1)\n",
    "\n",
    "ohe_items_staff = pd.get_dummies(items_df['staff_list'].apply(pd.Series).stack(), prefix='staff').sum(level=0)\n",
    "ohe_items_staff = ohe_items_staff[top_for_columns(items_df['staff_list'], 10, 'staff_')].drop('staff_', axis=1)\n",
    "ohe_items_genres = pd.get_dummies(items_df['genres_list'].apply(pd.Series).stack(), prefix='genre').sum(level=0)\n",
    "ohe_items_genres = ohe_items_genres[top_for_columns(items_df['genres_list'], 15, 'genre_')].drop('genre_', axis=1)\n",
    "ohe_items_country = pd.get_dummies(items_df['countries_list'].apply(pd.Series).stack(), prefix='country').sum(level=0)\n",
    "ohe_items_country = ohe_items_country[top_for_columns(items_df['countries_list'], 10, 'country_')].drop('country_', axis=1)\n",
    "\n",
    "items_ohe_df = items_df.item_id\n",
    "items_ohe_df = pd.concat([items_ohe_df, ohe_items_staff], axis=1)\n",
    "items_ohe_df = pd.concat([items_ohe_df, ohe_items_genres], axis=1)\n",
    "items_ohe_df = pd.concat([items_ohe_df, ohe_items_country], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td></td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>9960</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3245</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2950</th>\n",
       "      <td>10419</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>26989</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12081</th>\n",
       "      <td>24672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12079</th>\n",
       "      <td>32676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12077</th>\n",
       "      <td>34020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12076</th>\n",
       "      <td>28069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26845</th>\n",
       "      <td>18341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  count\n",
       "571             395\n",
       "104     9960     38\n",
       "179     3245     33\n",
       "2950   10419     32\n",
       "132    26989     32\n",
       "...      ...    ...\n",
       "12081  24672      1\n",
       "12079  32676      1\n",
       "12077  34020      1\n",
       "12076  28069      1\n",
       "26845  18341      1\n",
       "\n",
       "[26846 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counter для расчета количества элементов\n",
    "counter = Counter([item for sublist in items_df['staff_list'] for item in sublist])\n",
    "df_count = pd.DataFrame.from_dict(counter, orient='index', columns=['count'])\\\n",
    "                        .reset_index().rename(columns={'item':'item_id'})\\\n",
    "                        .sort_values(by=['count'], ascending=False)\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N users before: 182969\n",
      "N items before: 5237\n",
      "\n",
      "N users after: 38012\n",
      "N items after: 2484\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Собираем матрицу взаимодействий\n",
    "В датасете взаимодействий есть непопулярные фильмы и малоактивные пользователи. Кроме того, в таблице взаимодействий есть события с низким качеством взаимодействия - когда юзер начал смотреть фильм, но вскоре после начала просмотра выключил. \n",
    "\n",
    "Отфильтруем такие события*, малоактивных юзеров и непопулярные фильмы.\n",
    "\n",
    "_Можете не фильтровать такие события, тогда у вас будет больше негативных примеров._\n",
    "'''\n",
    "interactions_df = logs_train.groupby(['user_id', 'movie_id'], as_index=False).agg({'datetime':'max', 'duration':'sum'}).rename(columns={'movie_id':'item_id', 'duration':'total_dur', 'datetime':'last_watch_dt'})\n",
    "\n",
    "print(f\"N users before: {interactions_df.user_id.nunique()}\")\n",
    "print(f\"N items before: {interactions_df.item_id.nunique()}\\n\")\n",
    "\n",
    "# отфильтруем все события взаимодействий, в которых пользователь посмотрел\n",
    "# фильм менее чем на 35 процентов\n",
    "# замена на 10 минут\n",
    "interactions_df = interactions_df[interactions_df.total_dur > 1200]\n",
    "\n",
    "# соберем всех пользователей, которые посмотрели \n",
    "# больше 10 фильмов (можете выбрать другой порог)\n",
    "valid_users = []\n",
    "\n",
    "c = Counter(interactions_df.user_id)\n",
    "for user_id, entries in c.most_common():\n",
    "  if entries > 5:\n",
    "    valid_users.append(user_id)\n",
    "\n",
    "# и соберем все фильмы, которые посмотрели больше 10 пользователей\n",
    "valid_items = []\n",
    "\n",
    "c = Counter(interactions_df.item_id)\n",
    "for item_id, entries in c.most_common():\n",
    "  if entries > 50:\n",
    "    valid_items.append(item_id)\n",
    "\n",
    "# отбросим непопулярные фильмы и неактивных юзеров\n",
    "interactions_df = interactions_df[interactions_df.user_id.isin(valid_users)]\n",
    "interactions_df = interactions_df[interactions_df.item_id.isin(valid_items)]\n",
    "\n",
    "print(f\"N users after: {interactions_df.user_id.nunique()}\")\n",
    "print(f\"N items after: {interactions_df.item_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21323\n",
      "1596\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "После фильтрации может получиться так, что некоторые айтемы/юзеры есть в датасете взаимодействий, но при этом они отсутствуют в датасетах айтемов/юзеров или наоборот. \n",
    "Поэтому найдем id айтемов и id юзеров, которые есть во всех датасетах и оставим только их.\n",
    "'''\n",
    "common_users = set(interactions_df.user_id.unique()).intersection(set(users_ohe_df.user_id.unique()))\n",
    "common_items = set(interactions_df.item_id.unique()).intersection(set(items_ohe_df.item_id.unique()))\n",
    "\n",
    "print(len(common_users))\n",
    "print(len(common_items))\n",
    "\n",
    "interactions_df = interactions_df[interactions_df.item_id.isin(common_items)]\n",
    "interactions_df = interactions_df[interactions_df.user_id.isin(common_users)]\n",
    "\n",
    "items_ohe_df = items_ohe_df[items_ohe_df.item_id.isin(common_items)]\n",
    "users_ohe_df = users_ohe_df[users_ohe_df.user_id.isin(common_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем взаимодействия в матрицу user*item так, чтобы в строках этой матрицы были user_id, в столбцах - item_id, а на пересечениях строк и столбцов - единица, если пользователь взаимодействовал с айтемом и ноль, если нет.\n",
    "\n",
    "Такую матрицу удобно собирать в numpy array, однако нужно помнить, что numpy array индексируется порядковыми индексами, а нам же удобнее использовать item_id и user_id.\n",
    "\n",
    "Создадим некие внутренние индексы для user_id и item_id - uid и iid. Для этого просто соберем все user_id и item_id и пронумеруем их по порядку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>last_watch_dt</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>uid</th>\n",
       "      <th>iid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>2023-05-28 06:38:40.736898+03:00</td>\n",
       "      <td>31497</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>2023-05-31 20:35:46.239771+03:00</td>\n",
       "      <td>92347</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>2023-05-31 20:29:44.903470+03:00</td>\n",
       "      <td>16250</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>840</td>\n",
       "      <td>2023-05-31 20:43:47.751377+03:00</td>\n",
       "      <td>199321</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>927</td>\n",
       "      <td>2023-04-24 22:32:16.184274+03:00</td>\n",
       "      <td>3621</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id                    last_watch_dt  total_dur  uid  iid\n",
       "1         0       74 2023-05-28 06:38:40.736898+03:00      31497    0   10\n",
       "2         0      107 2023-05-31 20:35:46.239771+03:00      92347    0   14\n",
       "5         0      282 2023-05-31 20:29:44.903470+03:00      16250    0   53\n",
       "9         0      840 2023-05-31 20:43:47.751377+03:00     199321    0  159\n",
       "13        0      927 2023-04-24 22:32:16.184274+03:00       3621    0  171"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[\"uid\"] = interactions_df[\"user_id\"].astype(\"category\")\n",
    "interactions_df[\"uid\"] = interactions_df[\"uid\"].cat.codes\n",
    "\n",
    "interactions_df[\"iid\"] = interactions_df[\"item_id\"].astype(\"category\")\n",
    "interactions_df[\"iid\"] = interactions_df[\"iid\"].cat.codes\n",
    "\n",
    "print(sorted(interactions_df.iid.unique())[:5])\n",
    "print(sorted(interactions_df.uid.unique())[:5])\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец соберем и отнормируем матрицу взаимодействий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_vec = np.zeros((interactions_df.uid.nunique(), \n",
    "                             interactions_df.iid.nunique())) \n",
    "\n",
    "for user_id, item_id in zip(interactions_df.uid, interactions_df.iid):\n",
    "    interactions_vec[user_id, item_id] += 1\n",
    "\n",
    "\n",
    "res = interactions_vec.sum(axis=1)\n",
    "for i in range(len(interactions_vec)):\n",
    "    interactions_vec[i] /= res[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1596\n",
      "1596\n",
      "21323\n",
      "21323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(interactions_df.item_id.nunique())\n",
    "print(items_ohe_df.item_id.nunique())\n",
    "print(interactions_df.user_id.nunique())\n",
    "print(users_ohe_df.user_id.nunique())\n",
    "\n",
    "set(items_ohe_df.item_id.unique()) - set(interactions_df.item_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы можно было удобно превратить iid/uid в item_id/user_id и наоборот соберем словари \n",
    "\n",
    "{iid: item_id}, {uid: user_id} и {item_id: iid}, {user_id: uid}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_to_item_id = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"iid\").to_dict()[\"item_id\"]\n",
    "item_id_to_iid = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"item_id\").to_dict()[\"iid\"]\n",
    "\n",
    "uid_to_user_id = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"uid\").to_dict()[\"user_id\"]\n",
    "user_id_to_uid = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"user_id\").to_dict()[\"uid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И проиндексируем датасеты users_ohe_df и items_ohe_df по внутренним айди:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ohe_df[\"iid\"] = items_ohe_df[\"item_id\"].apply(lambda x: item_id_to_iid[x])\n",
    "items_ohe_df = items_ohe_df.set_index(\"iid\")\n",
    "\n",
    "users_ohe_df[\"uid\"] = users_ohe_df[\"user_id\"].apply(lambda x: user_id_to_uid.get(x, 'unknown'))\n",
    "users_ohe_df = users_ohe_df.set_index(\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSSM starter's pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем вектор юзера (anchor) и векторы двух айтемов - \"хорошего\" и \"плохого\" (positive и negative). Хороший айтем - это тот, который пользователь уже посмотрел, а в качестве плохого возьмем любой случайный айтем из датасета. Затем посчитаем расстояния:\n",
    "1. между вектором юзера и вектором \"хорошего\" айтема\n",
    "2. между вектором юзера и вектором \"плохого\" айтема\n",
    "\n",
    "_Значением функции потерь будет разность между первым и вторым расстоянием._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, n_dims=128, alpha=0.4):\n",
    "    # будем ожидать, что на вход функции прилетит три сконкатенированных \n",
    "    # вектора - вектор юзера и два вектора айтема\n",
    "    anchor = y_pred[:, 0:n_dims]\n",
    "    positive = y_pred[:, n_dims:n_dims*2]\n",
    "    negative = y_pred[:, n_dims*2:n_dims*3]\n",
    "\n",
    "    # считаем расстояния от вектора юзера до вектора хорошего айтема\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "    # и до плохого\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "\n",
    "    # считаем лосс\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = K.maximum(basic_loss, 0.0) # возвращаем ноль, если лосс отрицательный\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(items, users, interactions, batch_size=1024):\n",
    "    while True:\n",
    "        uid_meta = []\n",
    "        uid_interaction = []\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for _ in range(batch_size):\n",
    "            # берем рандомный uid\n",
    "            uid_i = randint(0, interactions.shape[0]-1)\n",
    "            # id хорошего айтема\n",
    "            pos_i = np.random.choice(range(interactions.shape[1]), p=interactions[uid_i])\n",
    "            # id плохого айтема\n",
    "            neg_i = np.random.choice(range(interactions.shape[1]))\n",
    "            # фичи юзера\n",
    "            uid_meta.append(users.iloc[uid_i])\n",
    "            # вектор айтемов, с которыми юзер взаимодействовал\n",
    "            uid_interaction.append(interactions_vec[uid_i])\n",
    "            # фичи хорошего айтема\n",
    "            pos.append(items.iloc[pos_i])\n",
    "            # фичи плохого айтема\n",
    "            neg.append(items.iloc[neg_i])\n",
    "            \n",
    "        yield [np.array(uid_meta), np.array(uid_interaction), np.array(pos), np.array(neg)], [np.array(uid_meta), np.array(uid_interaction)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вектор фичей юзера: (1024, 8)\n",
      "вектор взаимодействий юзера с айтемами: (1024, 1596)\n",
      "вектор 'хорошего' айтема: (1024, 442)\n",
      "вектор 'плохого' айтема: (1024, 442)\n",
      "\n",
      "вектор фичей юзера: (1024, 8)\n",
      "вектор взаимодействий юзера с айтемами: (1024, 1596)\n"
     ]
    }
   ],
   "source": [
    "# инициализируем генератор\n",
    "gen = generator(items=items_ohe_df.drop([\"item_id\"], axis=1), \n",
    "                users=users_ohe_df.drop([\"user_id\"], axis=1), \n",
    "                interactions=interactions_vec)\n",
    "\n",
    "ret = next(gen)\n",
    "\n",
    "\n",
    "print(f\"вектор фичей юзера: {ret[0][0].shape}\")\n",
    "print(f\"вектор взаимодействий юзера с айтемами: {ret[0][1].shape}\")\n",
    "print(f\"вектор 'хорошего' айтема: {ret[0][2].shape}\")\n",
    "print(f\"вектор 'плохого' айтема: {ret[0][3].shape}\")\n",
    "print()\n",
    "print(f\"вектор фичей юзера: {ret[1][0].shape}\")\n",
    "print(f\"вектор взаимодействий юзера с айтемами: {ret[1][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы обучить модель используя триплет лосс, нам нужно получить три вектора - вектор юзера, вектор \"хорошего\" айтема и вектор \"плохого\" айтема. Для этого нам нужно две модели (\"хороший\" и \"плохой\" айтем будут семплироваться одной и той же моделью).  \n",
    "\n",
    "Модель юзера будет иметь два входа:\n",
    "- вход для фичей юзера (фичи из users_ohe_df)\n",
    "- вход для вектора айтемов, которые посмотрел юзер (строка interactions_vec, которая соответствует uid конкретного юзера)\n",
    "\n",
    "Выход модели юзера будет размерностью N_FACTORS.\n",
    "\n",
    "У модели айтема будет один вход для фичей айтема (из items_ohe_df) и один выход также размерностью N_FACTORS.\n",
    "\n",
    "Общая архитектура будет вот такой: \n",
    "- есть модель юзера и модель айтема\n",
    "- обе модели семплируют юзер и айтем-фичи во внутреннее пространство размерностью N_FACTORS\n",
    "- модель айтема семплирует два айтема - \"хороший\" и \"плохой\"\n",
    "- в итоге получается три вектора размерностью N_FACTORS (вектор юзера, вектор \"хорошего\" айтема и вектор \"плохого\" айтема)\n",
    "- затем полученные векторы конкатенируются, по ним считается triplet loss\n",
    "- profit\n",
    "\n",
    "Для того, чтобы собрать модель, помимо размерности внутреннего пространства, нам нужно знать еще размерность вектора юзера и вектора айтема. Зададим их сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_FACTORS: 128\n",
      "ITEM_MODEL_SHAPE: (442,)\n",
      "USER_META_MODEL_SHAPE: (8,)\n",
      "USER_INTERACTION_MODEL_SHAPE: (1596,)\n"
     ]
    }
   ],
   "source": [
    "N_FACTORS = 128\n",
    "\n",
    "# в датасетах есть столбец user_id/item_id, помним, что он не является фичей для обучения!\n",
    "ITEM_MODEL_SHAPE = (items_ohe_df.drop([\"item_id\"], axis=1).shape[1], ) \n",
    "USER_META_MODEL_SHAPE = (users_ohe_df.drop([\"user_id\"], axis=1).shape[1], )\n",
    "\n",
    "USER_INTERACTION_MODEL_SHAPE = (interactions_vec.shape[1], )\n",
    "\n",
    "print(f\"N_FACTORS: {N_FACTORS}\")\n",
    "print(f\"ITEM_MODEL_SHAPE: {ITEM_MODEL_SHAPE}\")\n",
    "print(f\"USER_META_MODEL_SHAPE: {USER_META_MODEL_SHAPE}\")\n",
    "print(f\"USER_INTERACTION_MODEL_SHAPE: {USER_INTERACTION_MODEL_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_model(n_factors=N_FACTORS):\n",
    "    # входной слой\n",
    "    inp = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "    \n",
    "    # полносвязный слой\n",
    "    layer_1 = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                               kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                               activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp)\n",
    "\n",
    "    # делаем residual connection - складываем два слоя, \n",
    "    # чтобы градиенты не затухали во время обучения\n",
    "    layer_2 = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(layer_1)\n",
    "    \n",
    "    add = keras.layers.Add()([layer_1, layer_2])\n",
    "    \n",
    "    # выходной слой\n",
    "    out = keras.layers.Dense(N_FACTORS, activation='linear', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(add)\n",
    "    \n",
    "    return keras.models.Model(inp, out)\n",
    "\n",
    "\n",
    "def user_model(n_factors=N_FACTORS):\n",
    "    # входной слой для вектора фичей юзера (из users_ohe_df)\n",
    "    inp_meta = keras.layers.Input(shape=USER_META_MODEL_SHAPE)\n",
    "    # входной слой для вектора просмотров (из iteractions_vec)\n",
    "    inp_interaction = keras.layers.Input(shape=USER_INTERACTION_MODEL_SHAPE)\n",
    "\n",
    "    # полносвязный слой\n",
    "    layer_1_meta = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp_meta)\n",
    "\n",
    "    layer_1_interaction = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp_interaction)\n",
    "\n",
    "    # делаем residual connection - складываем два слоя,\n",
    "    # чтобы градиенты не затухали во время обучения\n",
    "    layer_2_meta = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(layer_1_meta)\n",
    "    \n",
    "\n",
    "    add = keras.layers.Add()([layer_1_meta, layer_2_meta])\n",
    "    \n",
    "    # конкатенируем вектор фичей с вектором просмотров\n",
    "    concat_meta_interaction = keras.layers.Concatenate()([add, layer_1_interaction])\n",
    "    \n",
    "    # выходной слой\n",
    "    out = keras.layers.Dense(N_FACTORS, activation='linear', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(concat_meta_interaction)\n",
    "    \n",
    "    return keras.models.Model([inp_meta, inp_interaction], out)\n",
    "\n",
    "# инициализируем модели юзера и айтема\n",
    "i2v = item_model()\n",
    "u2v = user_model()\n",
    "\n",
    "# вход для вектора фичей юзера (из users_ohe_df)\n",
    "ancor_meta_in = keras.layers.Input(shape=USER_META_MODEL_SHAPE)\n",
    "# вход для вектора просмотра юзера (из interactions_vec)\n",
    "ancor_interaction_in = keras.layers.Input(shape=USER_INTERACTION_MODEL_SHAPE)\n",
    "\n",
    "# вход для вектора \"хорошего\" айтема\n",
    "pos_in = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "# вход для вектора \"плохого\" айтема\n",
    "neg_in = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "\n",
    "# получаем вектор юзера\n",
    "ancor = u2v([ancor_meta_in, ancor_interaction_in])\n",
    "# получаем вектор \"хорошего\" айтема\n",
    "pos = i2v(pos_in)\n",
    "# получаем вектор \"плохого\" айтема\n",
    "neg = i2v(neg_in)\n",
    "\n",
    "# конкатенируем полученные векторы\n",
    "res = keras.layers.Concatenate(name=\"concat_ancor_pos_neg\")([ancor, pos, neg])\n",
    "\n",
    "# собираем модель\n",
    "model = keras.models.Model([ancor_meta_in, ancor_interaction_in, pos_in, neg_in], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'recsys_resnet_linear'\n",
    "\n",
    "# логируем процесс обучения в тензорборд\n",
    "t_board = keras.callbacks.TensorBoard(log_dir=f'runs/{model_name}')\n",
    "\n",
    "# уменьшаем learning_rate, если лосс долго не уменьшается (в течение двух эпох)\n",
    "decay = keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, factor=0.8, verbose=1)\n",
    "\n",
    "# сохраняем модель после каждой эпохи, если лосс уменьшился\n",
    "check = keras.callbacks.ModelCheckpoint(filepath=model_name + '/epoch{epoch}-{loss:.2f}.h5', monitor=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# компилируем модель, используем оптимайзер Adam и triplet loss\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss=triplet_loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_33\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)       [(None, 442)]                0         []                            \n",
      "                                                                                                  \n",
      " dense_91 (Dense)            (None, 128)                  56576     ['input_68[0][0]']            \n",
      "                                                                                                  \n",
      " dense_92 (Dense)            (None, 128)                  16384     ['dense_91[0][0]']            \n",
      "                                                                                                  \n",
      " add_26 (Add)                (None, 128)                  0         ['dense_91[0][0]',            \n",
      "                                                                     'dense_92[0][0]']            \n",
      "                                                                                                  \n",
      " dense_93 (Dense)            (None, 128)                  16384     ['add_26[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 89344 (349.00 KB)\n",
      "Trainable params: 89344 (349.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# модель айтема\n",
    "item_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_34\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_69 (InputLayer)       [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_94 (Dense)            (None, 128)                  1024      ['input_69[0][0]']            \n",
      "                                                                                                  \n",
      " dense_96 (Dense)            (None, 128)                  16384     ['dense_94[0][0]']            \n",
      "                                                                                                  \n",
      " input_70 (InputLayer)       [(None, 2894)]               0         []                            \n",
      "                                                                                                  \n",
      " add_27 (Add)                (None, 128)                  0         ['dense_94[0][0]',            \n",
      "                                                                     'dense_96[0][0]']            \n",
      "                                                                                                  \n",
      " dense_95 (Dense)            (None, 128)                  370432    ['input_70[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenat  (None, 256)                  0         ['add_27[0][0]',              \n",
      " e)                                                                  'dense_95[0][0]']            \n",
      "                                                                                                  \n",
      " dense_97 (Dense)            (None, 128)                  32768     ['concatenate_13[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 420608 (1.60 MB)\n",
      "Trainable params: 420608 (1.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# модель юзера\n",
    "user_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_64 (InputLayer)       [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_65 (InputLayer)       [(None, 2894)]               0         []                            \n",
      "                                                                                                  \n",
      " input_66 (InputLayer)       [(None, 442)]                0         []                            \n",
      "                                                                                                  \n",
      " input_67 (InputLayer)       [(None, 442)]                0         []                            \n",
      "                                                                                                  \n",
      " model_31 (Functional)       (None, 128)                  420608    ['input_64[0][0]',            \n",
      "                                                                     'input_65[0][0]']            \n",
      "                                                                                                  \n",
      " model_30 (Functional)       (None, 128)                  89344     ['input_66[0][0]',            \n",
      "                                                                     'input_67[0][0]']            \n",
      "                                                                                                  \n",
      " concat_ancor_pos_neg (Conc  (None, 384)                  0         ['model_31[0][0]',            \n",
      " atenate)                                                            'model_30[0][0]',            \n",
      "                                                                     'model_30[1][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 509952 (1.95 MB)\n",
      "Trainable params: 509952 (1.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# общая модель\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 4s 30ms/step - loss: 0.5822 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.3219 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2604 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2280 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2148 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.2120 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.1977 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.1911 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1952 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.1842 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 0.1766 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1790 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.1716 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.1685 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.1664 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.1698 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1667\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1667 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1524 - lr: 8.0000e-04\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1584 - lr: 8.0000e-04\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1552\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1552 - lr: 8.0000e-04\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1540 - lr: 6.4000e-04\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.1433 - lr: 6.4000e-04\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1490 - lr: 6.4000e-04\n",
      "Epoch 24/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1481\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1483 - lr: 6.4000e-04\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 4s 38ms/step - loss: 0.1385 - lr: 5.1200e-04\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1394 - lr: 5.1200e-04\n",
      "Epoch 27/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1424\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.1425 - lr: 5.1200e-04\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 3s 35ms/step - loss: 0.1355 - lr: 4.0960e-04\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.1359 - lr: 4.0960e-04\n",
      "Epoch 30/30\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1361\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.1362 - lr: 4.0960e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e6e84cfa60>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# начинаем обучение, не забывая дропнуть столбцы item_id и user_id \n",
    "# из датафреймов при инициализации генератора.\n",
    "\n",
    "# batch_size можно (и лучше) поставить побольше, если вы не органичены в ресурсах\n",
    "\n",
    "model.fit(generator(items=items_ohe_df.drop([\"item_id\"], axis=1), \n",
    "                    users=users_ohe_df.drop([\"user_id\"], axis=1), \n",
    "                    interactions=interactions_vec,\n",
    "                    batch_size=64), \n",
    "          steps_per_epoch=100, \n",
    "          epochs=30, \n",
    "          initial_epoch=0,\n",
    "          callbacks=[decay, t_board, check]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (пример на слуяайном)\n",
    "\n",
    "Отлично! Мы подготовили данные, собрали модель по архитектуре DSSM и обучили ее.\n",
    "Теперь возьмем случайного юзера и случайный айтем. Как понять, насколько этот айтем релевантен юзеру?\n",
    "\n",
    "Нужно:\n",
    "- получить вектор айтема;\n",
    "- получить вектор юзера;\n",
    "- посчитать расстояние между ними.\n",
    "\n",
    "Это расстояние и есть мера релевантности.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3160759]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# берем рандомного юзера\n",
    "rand_uid = np.random.choice(list(users_ohe_df.index))\n",
    "\n",
    "# получаем фичи юзера и вектор его просмотров айтемов\n",
    "user_meta_feats = users_ohe_df.drop([\"user_id\"], axis=1).iloc[rand_uid.astype(int)]\n",
    "user_interaction_vec = interactions_vec[rand_uid.astype(int)]\n",
    "\n",
    "# берем рандомный айтем\n",
    "rand_iid = np.random.choice(list(items_ohe_df.index))\n",
    "# получаем фичи айтема\n",
    "item_feats = items_ohe_df.drop([\"item_id\"], axis=1).iloc[rand_iid]\n",
    "\n",
    "# получаем вектор юзера\n",
    "user_vec = u2v.predict([np.array(user_meta_feats).reshape(1, -1), \n",
    "                        np.array(user_interaction_vec).reshape(1, -1)])\n",
    "\n",
    "# и вектор айтема\n",
    "item_vec = i2v.predict(np.array(item_feats).reshape(1, -1))\n",
    "\n",
    "# считаем расстояние между вектором юзера и вектором айтема\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ED\n",
    "\n",
    "ED(user_vec, item_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# получаем фичи всех айтемов\n",
    "items_feats = items_ohe_df.drop([\"item_id\"], axis=1).to_numpy()\n",
    "# получаем векторы всех айтемов\n",
    "items_vecs = i2v.predict(items_feats)\n",
    "\n",
    "# считаем расстояния\n",
    "dists = ED(user_vec, items_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 267,  180,  808, 2088, 1263,  242, 2430,  254, 1823, 1997,  801,\n",
       "       1050,  799,  175, 1659, 2701,  956, 2213, 1368, 2354], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_iids = np.argsort(dists, axis=1)[0][:20]\n",
    "top5_iids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось конвертировать внутренние iid в ~~id здорового человека~~ item_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_item_ids = [iid_to_item_id[iid] for iid in top5_iids]\n",
    "df_pred_u = pd.DataFrame({'item_id': top5_item_ids})\n",
    "df_pred_u['rank'] = df_pred_u.index+1\n",
    "df_pred_u['user_id'] = rand_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "475                                          2012\n",
       "484                             Аватар: Путь воды\n",
       "658                                         Мумия\n",
       "685                                  Чёрная Вдова\n",
       "728     Доктор Стрэндж: В мультивселенной безумия\n",
       "2052                                       Дэдпул\n",
       "2057                                  Чёрный Адам\n",
       "2074                                      Морбиус\n",
       "2460                               Крепкий орешек\n",
       "2679                                       Вечныe\n",
       "3213                           Мумия возвращается\n",
       "3493              Отряд самоубийц: Миссия навылет\n",
       "4178                       Бегущий по лезвию 2049\n",
       "4603                   Kingsman: Секретная служба\n",
       "5042                       Хроники хищных городов\n",
       "5281                Шан-Чи и легенда десяти колец\n",
       "5584                            Миссия «Серенити»\n",
       "5918                           Тор: Любовь и гром\n",
       "6099           G. I. Joe. Бросок кобры: Снейк Айз\n",
       "6758                    Люди Икс: Последняя битва\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_titles = items_df.loc[items_df.item_id.isin(top5_item_ids)].name\n",
    "recommended_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference для всей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 2ms/step\n",
      "667/667 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances as ED\n",
    "\n",
    "# Получаем фичи всех айтемов и векторы всех айтемов заранее\n",
    "items_feats = items_ohe_df.drop([\"item_id\"], axis=1).to_numpy()\n",
    "items_vecs = i2v.predict(items_feats)\n",
    "\n",
    "# Получаем фичи юзеров и векторы их просмотров айтемов\n",
    "#user_meta_feats = users_ohe_df.drop('unknown', axis=0).drop([\"user_id\"], axis=1).to_numpy()\n",
    "user_meta_feats = users_ohe_df.drop([\"user_id\"], axis=1).to_numpy()\n",
    "\n",
    "user_interaction_vecs = interactions_vec  # Нет необходимости использовать to_numpy()\n",
    "\n",
    "# Получаем векторы юзеров\n",
    "user_vecs = u2v.predict([user_meta_feats, user_interaction_vecs])\n",
    "\n",
    "# Считаем расстояния между векторами юзеров и векторами всех айтемов\n",
    "dists = ED(user_vecs, items_vecs)\n",
    "\n",
    "# Находим ТОП-20 айтемов для каждого пользователя\n",
    "top20_iids = np.argsort(dists, axis=1)[:, :20]\n",
    "top20_item_ids = np.vectorize(iid_to_item_id.get)(top20_iids)  # Применяем маппинг iid в item_id\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "df_pred = pd.DataFrame({\n",
    "    'user_id': np.repeat(users_ohe_df.index, 20)[:len(top20_item_ids.flatten())],\n",
    "    'item_id': top20_item_ids.flatten(),\n",
    "    'rank': np.tile(np.arange(1, 21), len(users_ohe_df))[:len(top20_item_ids.flatten())]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расчет метрики МАР"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006182297656765647"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline == 0.0218218110206651\n",
    "# map_v2(logs_test_norm, bl_pred, 20)\n",
    "# фичи айтемов #1 == 0.00586553605484825\n",
    "# фичи айтемов #1 == 0.005702631349202798; >1800sec; >10 films; >20 users\n",
    "# фичи айтемов #2 == 0.004860998200328909; >600sec;  >2 films;  >10 users; topval staff=10, genres=15, counties=10\n",
    "# фичи айтемов #2 == 0.005121920502313525; >1800sec; >10 films; >20 users; topval staff=20, genres=20, counties=20\n",
    "# фичи айтемов #2 == 0.0051762269717752175;>1800sec; >10 films; >20 users; topval staff=5, genres=10, counties=5\n",
    "# фичи айтемов #2 == 0.005103638740674767; >1800sec; >10 films; >100 users; topval staff=10, genres=15, counties=10\n",
    "# фичи айтемов #2 == 0.00616865866360225;  >1800sec; >10 films; >20 users; topval staff=10, genres=15, counties=10\n",
    "# фичи айтемов #2 == 0.006182297656765647; >1200sec; >5 films;  >50  users; topval staff=10, genres=15, counties=10\n",
    "map_v2(logs_test_norm, df_pred, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(df_true, df_pred, k=20, target_col='rank'):\n",
    "\n",
    "    # Объединение данных по 'user_id' и 'item_id'\n",
    "\n",
    "    df = df_true.set_index(['user_id', 'item_id']).join(df_pred.set_index(['user_id', 'item_id']))\n",
    "\n",
    "    # Отбор пользователей, которые есть и в обучающей, и в тестовой выборке\n",
    "    common_users = set(df_true['user_id']).intersection(set(df_pred['user_id']))\n",
    "    df = df[df.index.get_level_values('user_id').isin(common_users)]\n",
    "\n",
    "    # Сортировка данных по 'user_id' и предсказаниям\n",
    "    df = df.sort_values(by=['user_id', target_col], ascending=[True, False])\n",
    "\n",
    "    # Инициализация переменных для расчета MAP\n",
    "    total_precision = 0.0\n",
    "    users_count = 0\n",
    "\n",
    "    # Расчет MAP для каждого пользователя\n",
    "    for user_id, user_df in df.groupby(level='user_id'):\n",
    "        user_precision = 0.0\n",
    "        relevant_items = 0\n",
    "\n",
    "        for i, (_, row) in enumerate(user_df.head(k).iterrows(), 1):\n",
    "            # Рассчитываем W = P / (N * R)\n",
    "            precision = row['interaction'] / i\n",
    "            user_precision += precision\n",
    "            relevant_items += row['interaction']\n",
    "\n",
    "        if relevant_items > 0:\n",
    "            user_precision /= relevant_items  # Усреднение для пользователя\n",
    "            total_precision += user_precision\n",
    "            users_count += 1\n",
    "\n",
    "\n",
    "    # Расчет среднего значения MAP по всем пользователям\n",
    "    if users_count > 0:\n",
    "        mean_ap = total_precision / users_count\n",
    "        return mean_ap\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk (actual, predicted, k=10):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def map_v2(df_true, df_pred, k=10):\n",
    "    test_items = df_true.groupby('user_id')['item_id'].apply(list).reset_index(name='item_list_test')\n",
    "    pred_items = df_pred.sort_values(by='rank').groupby('user_id')['item_id'].apply(list).reset_index(name='item_list_pred')\n",
    "    merget_array = pd.merge(test_items, pred_items, on='user_id', how='inner')\n",
    "    return mapk(merget_array['item_list_test'], merget_array['item_list_pred'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## скрыть расчет метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5990594263288112"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(logs_test_norm, df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796000244990886"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "mean_average_precision(logs_test_norm, bl_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0218218110206651"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline == 0.0218218110206651\n",
    "map_v2(logs_test_norm, bl_pred, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
