{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "from random import randint, random\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(r\"C:\\Users\\vazhitenev\\PycharmProjects\\Contest2023\\oneproj\\train\\movies.csv\")\n",
    "countries = pd.read_csv(r\"C:\\Users\\vazhitenev\\PycharmProjects\\Contest2023\\oneproj\\train\\countries.csv\")\n",
    "genres = pd.read_csv(r\"C:\\Users\\vazhitenev\\PycharmProjects\\Contest2023\\oneproj\\train\\genres.csv\")\n",
    "staff = pd.read_csv(r\"C:\\Users\\vazhitenev\\PycharmProjects\\Contest2023\\oneproj\\train\\staff.csv\")\n",
    "logs = pd.read_csv(r\"C:\\Users\\vazhitenev\\PycharmProjects\\Contest2023\\oneproj\\train\\logs.csv\")\n",
    "#удаляем из списка фильмо те, которые не были опубликованы\n",
    "movies = movies[~movies['date_publication'].isnull()]\n",
    "# переводим тип float to intager\n",
    "logs['movie_id'] = logs['movie_id'].astype(int)\n",
    "logs['duration'] = logs['duration'].astype(int)\n",
    "logs['datetime'] = pd.to_datetime(logs['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_quantile_(0.0, 51845.0]</th>\n",
       "      <th>movie_quantile_(51845.0, 103689.0]</th>\n",
       "      <th>movie_quantile_(103689.0, 155533.0]</th>\n",
       "      <th>movie_quantile_(155533.0, 207377.0]</th>\n",
       "      <th>duration_quantile_(0.0, 51845.0]</th>\n",
       "      <th>duration_quantile_(51845.0, 103689.0]</th>\n",
       "      <th>duration_quantile_(103689.0, 155533.0]</th>\n",
       "      <th>duration_quantile_(155533.0, 207377.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_quantile_(0.0, 51845.0]  movie_quantile_(51845.0, 103689.0]  \\\n",
       "0        0                              0                                   0   \n",
       "1        1                              0                                   0   \n",
       "2        2                              0                                   0   \n",
       "3        3                              0                                   0   \n",
       "4        4                              0                                   0   \n",
       "\n",
       "   movie_quantile_(103689.0, 155533.0]  movie_quantile_(155533.0, 207377.0]  \\\n",
       "0                                    0                                    1   \n",
       "1                                    0                                    1   \n",
       "2                                    0                                    1   \n",
       "3                                    0                                    1   \n",
       "4                                    0                                    1   \n",
       "\n",
       "   duration_quantile_(0.0, 51845.0]  duration_quantile_(51845.0, 103689.0]  \\\n",
       "0                                 0                                      0   \n",
       "1                                 0                                      0   \n",
       "2                                 0                                      0   \n",
       "3                                 0                                      0   \n",
       "4                                 0                                      0   \n",
       "\n",
       "   duration_quantile_(103689.0, 155533.0]  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "\n",
       "   duration_quantile_(155533.0, 207377.0]  \n",
       "0                                       1  \n",
       "1                                       1  \n",
       "2                                       1  \n",
       "3                                       1  \n",
       "4                                       1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим фичи пользователей\n",
    "users_df = logs.groupby('user_id', as_index=False).agg({'movie_id':'nunique', 'duration':'sum'}).rename(columns={'movie_id':'movie_count', 'duration':'sum_duration'})\n",
    "users_df['movie_quantile']   =pd.qcut(users_df['movie_count'].rank(method='first'), q=4, precision=0)\n",
    "users_df['duration_quantile']=pd.qcut(users_df['sum_duration'].rank(method='first'), q=4, precision=0)\n",
    "user_cat_feats = [\"movie_quantile\", \"duration_quantile\"]\n",
    "\n",
    "# из исходного датафрейма оставим только item_id - этот признак нам понадобится позже\n",
    "# для того, чтобы маппить айтемы из датафрейма с фильмами с айтемами \n",
    "# из датафрейма с взаимодействиями\n",
    "users_ohe_df = users_df.user_id\n",
    "for feat in user_cat_feats:\n",
    "  # получаем датафрейм с one-hot encoding для каждой категориальной фичи\n",
    "  ohe_feat_df = pd.get_dummies(users_df[feat], prefix=feat)\n",
    "  # конкатенируем ohe-hot датафрейм с датафреймом, \n",
    "  # который мы получили на предыдущем шаге\n",
    "  users_ohe_df = pd.concat([users_ohe_df, ohe_feat_df], axis=1)\n",
    "\n",
    "users_ohe_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>year_1895-01-01</th>\n",
       "      <th>year_1906-01-01</th>\n",
       "      <th>year_1925-01-01</th>\n",
       "      <th>year_1934-01-01</th>\n",
       "      <th>year_1937-01-01</th>\n",
       "      <th>year_1938-01-01</th>\n",
       "      <th>year_1939-01-01</th>\n",
       "      <th>year_1940-01-01</th>\n",
       "      <th>year_1941-01-01</th>\n",
       "      <th>...</th>\n",
       "      <th>countries_[81, 121, 102, 146]</th>\n",
       "      <th>countries_[81, 121]</th>\n",
       "      <th>countries_[81]</th>\n",
       "      <th>countries_[83, 102]</th>\n",
       "      <th>countries_[83, 242, 102]</th>\n",
       "      <th>countries_[83]</th>\n",
       "      <th>countries_[84]</th>\n",
       "      <th>countries_[85, 122, 121, 102]</th>\n",
       "      <th>countries_[93, 109]</th>\n",
       "      <th>countries_[]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  year_1895-01-01  year_1906-01-01  year_1925-01-01  \\\n",
       "0        0                0                0                0   \n",
       "3        3                0                0                0   \n",
       "4        4                0                0                0   \n",
       "5        5                0                0                0   \n",
       "6        6                0                0                0   \n",
       "\n",
       "   year_1934-01-01  year_1937-01-01  year_1938-01-01  year_1939-01-01  \\\n",
       "0                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "5                0                0                0                0   \n",
       "6                0                0                0                0   \n",
       "\n",
       "   year_1940-01-01  year_1941-01-01  ...  countries_[81, 121, 102, 146]  \\\n",
       "0                0                0  ...                              0   \n",
       "3                0                0  ...                              0   \n",
       "4                0                0  ...                              0   \n",
       "5                0                0  ...                              0   \n",
       "6                0                0  ...                              0   \n",
       "\n",
       "   countries_[81, 121]  countries_[81]  countries_[83, 102]  \\\n",
       "0                    0               0                    0   \n",
       "3                    0               0                    0   \n",
       "4                    0               0                    0   \n",
       "5                    0               0                    0   \n",
       "6                    0               0                    0   \n",
       "\n",
       "   countries_[83, 242, 102]  countries_[83]  countries_[84]  \\\n",
       "0                         0               0               0   \n",
       "3                         0               0               0   \n",
       "4                         0               0               0   \n",
       "5                         0               0               0   \n",
       "6                         0               0               0   \n",
       "\n",
       "   countries_[85, 122, 121, 102]  countries_[93, 109]  countries_[]  \n",
       "0                              0                    0             0  \n",
       "3                              0                    0             0  \n",
       "4                              0                    0             0  \n",
       "5                              0                    0             0  \n",
       "6                              0                    0             0  \n",
       "\n",
       "[5 rows x 1303 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим фичи айтемов\n",
    "items_df = movies.rename(columns={'id':'item_id'})\n",
    "\n",
    "item_cat_feats = ['year', 'genres', 'countries']\n",
    "\n",
    "items_ohe_df = items_df.item_id\n",
    "\n",
    "for feat in item_cat_feats:\n",
    "  ohe_feat_df = pd.get_dummies(items_df[feat], prefix=feat)\n",
    "  items_ohe_df = pd.concat([items_ohe_df, ohe_feat_df], axis=1) \n",
    "\n",
    "items_ohe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N users before: 207377\n",
      "N items before: 5283\n",
      "\n",
      "N users after: 76429\n",
      "N items after: 4343\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Собираем матрицу взаимодействий\n",
    "В датасете взаимодействий есть непопулярные фильмы и малоактивные пользователи. Кроме того, в таблице взаимодействий есть события с низким качеством взаимодействия - когда юзер начал смотреть фильм, но вскоре после начала просмотра выключил. \n",
    "\n",
    "Отфильтруем такие события*, малоактивных юзеров и непопулярные фильмы.\n",
    "\n",
    "_Можете не фильтровать такие события, тогда у вас будет больше негативных примеров._\n",
    "'''\n",
    "interactions_df = logs.groupby(['user_id', 'movie_id'], as_index=False).agg({'datetime':'max', 'duration':'sum'}).rename(columns={'movie_id':'item_id', 'duration':'total_dur', 'datetime':'last_watch_dt'})\n",
    "\n",
    "print(f\"N users before: {interactions_df.user_id.nunique()}\")\n",
    "print(f\"N items before: {interactions_df.item_id.nunique()}\\n\")\n",
    "\n",
    "# отфильтруем все события взаимодействий, в которых пользователь посмотрел\n",
    "# фильм менее чем на 35 процентов\n",
    "# замена на 10 минут\n",
    "interactions_df = interactions_df[interactions_df.total_dur > 600]\n",
    "\n",
    "# соберем всех пользователей, которые посмотрели \n",
    "# больше 10 фильмов (можете выбрать другой порог)\n",
    "valid_users = []\n",
    "\n",
    "c = Counter(interactions_df.user_id)\n",
    "for user_id, entries in c.most_common():\n",
    "  if entries > 2:\n",
    "    valid_users.append(user_id)\n",
    "\n",
    "# и соберем все фильмы, которые посмотрели больше 10 пользователей\n",
    "valid_items = []\n",
    "\n",
    "c = Counter(interactions_df.item_id)\n",
    "for item_id, entries in c.most_common():\n",
    "  if entries > 5:\n",
    "    valid_items.append(item_id)\n",
    "\n",
    "# отбросим непопулярные фильмы и неактивных юзеров\n",
    "interactions_df = interactions_df[interactions_df.user_id.isin(valid_users)]\n",
    "interactions_df = interactions_df[interactions_df.item_id.isin(valid_items)]\n",
    "\n",
    "print(f\"N users after: {interactions_df.user_id.nunique()}\")\n",
    "print(f\"N items after: {interactions_df.item_id.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76429\n",
      "3963\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "После фильтрации может получиться так, что некоторые айтемы/юзеры есть в датасете взаимодействий, но при этом они отсутствуют в датасетах айтемов/юзеров или наоборот. \n",
    "Поэтому найдем id айтемов и id юзеров, которые есть во всех датасетах и оставим только их.\n",
    "'''\n",
    "common_users = set(interactions_df.user_id.unique()).intersection(set(users_ohe_df.user_id.unique()))\n",
    "common_items = set(interactions_df.item_id.unique()).intersection(set(items_ohe_df.item_id.unique()))\n",
    "\n",
    "print(len(common_users))\n",
    "print(len(common_items))\n",
    "\n",
    "interactions_df = interactions_df[interactions_df.item_id.isin(common_items)]\n",
    "interactions_df = interactions_df[interactions_df.user_id.isin(common_users)]\n",
    "\n",
    "items_ohe_df = items_ohe_df[items_ohe_df.item_id.isin(common_items)]\n",
    "users_ohe_df = users_ohe_df[users_ohe_df.user_id.isin(common_users)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем взаимодействия в матрицу user*item так, чтобы в строках этой матрицы были user_id, в столбцах - item_id, а на пересечениях строк и столбцов - единица, если пользователь взаимодействовал с айтемом и ноль, если нет.\n",
    "\n",
    "Такую матрицу удобно собирать в numpy array, однако нужно помнить, что numpy array индексируется порядковыми индексами, а нам же удобнее использовать item_id и user_id.\n",
    "\n",
    "Создадим некие внутренние индексы для user_id и item_id - uid и iid. Для этого просто соберем все user_id и item_id и пронумеруем их по порядку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>last_watch_dt</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>uid</th>\n",
       "      <th>iid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2023-06-12 18:28:52.927833+03:00</td>\n",
       "      <td>8690</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>2023-06-14 12:16:18.451369+03:00</td>\n",
       "      <td>36183</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>2023-06-14 18:47:34.563849+03:00</td>\n",
       "      <td>110996</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>2023-04-18 11:59:47.414031+03:00</td>\n",
       "      <td>2286</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>2023-06-05 15:35:02.474099+03:00</td>\n",
       "      <td>934</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id                    last_watch_dt  total_dur  uid  iid\n",
       "0        0       12 2023-06-12 18:28:52.927833+03:00       8690    0    4\n",
       "1        0       74 2023-06-14 12:16:18.451369+03:00      36183    0   29\n",
       "2        0      107 2023-06-14 18:47:34.563849+03:00     110996    0   43\n",
       "4        0      165 2023-04-18 11:59:47.414031+03:00       2286    0   81\n",
       "5        0      190 2023-06-05 15:35:02.474099+03:00        934    0   93"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df[\"uid\"] = interactions_df[\"user_id\"].astype(\"category\")\n",
    "interactions_df[\"uid\"] = interactions_df[\"uid\"].cat.codes\n",
    "\n",
    "interactions_df[\"iid\"] = interactions_df[\"item_id\"].astype(\"category\")\n",
    "interactions_df[\"iid\"] = interactions_df[\"iid\"].cat.codes\n",
    "\n",
    "print(sorted(interactions_df.iid.unique())[:5])\n",
    "print(sorted(interactions_df.uid.unique())[:5])\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец соберем и отнормируем матрицу взаимодействий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_vec = np.zeros((interactions_df.uid.nunique(), \n",
    "                             interactions_df.iid.nunique())) \n",
    "\n",
    "for user_id, item_id in zip(interactions_df.uid, interactions_df.iid):\n",
    "    interactions_vec[user_id, item_id] += 1\n",
    "\n",
    "\n",
    "res = interactions_vec.sum(axis=1)\n",
    "for i in range(len(interactions_vec)):\n",
    "    interactions_vec[i] /= res[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3963\n",
      "3963\n",
      "76298\n",
      "76429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(interactions_df.item_id.nunique())\n",
    "print(items_ohe_df.item_id.nunique())\n",
    "print(interactions_df.user_id.nunique())\n",
    "print(users_ohe_df.user_id.nunique())\n",
    "\n",
    "set(items_ohe_df.item_id.unique()) - set(interactions_df.item_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы можно было удобно превратить iid/uid в item_id/user_id и наоборот соберем словари \n",
    "\n",
    "{iid: item_id}, {uid: user_id} и {item_id: iid}, {user_id: uid}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_to_item_id = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"iid\").to_dict()[\"item_id\"]\n",
    "item_id_to_iid = interactions_df[[\"iid\", \"item_id\"]].drop_duplicates().set_index(\"item_id\").to_dict()[\"iid\"]\n",
    "\n",
    "uid_to_user_id = interactions_df[[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"uid\").to_dict()[\"user_id\"]\n",
    "user_id_to_uid =  [[\"uid\", \"user_id\"]].drop_duplicates().set_index(\"user_id\").to_dict()[\"uid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И проиндексируем датасеты users_ohe_df и items_ohe_df по внутренним айди:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ohe_df[\"iid\"] = items_ohe_df[\"item_id\"].apply(lambda x: item_id_to_iid[x])\n",
    "items_ohe_df = items_ohe_df.set_index(\"iid\")\n",
    "\n",
    "users_ohe_df[\"uid\"] = users_ohe_df[\"user_id\"].apply(lambda x: user_id_to_uid.get(x, 'unknown'))\n",
    "users_ohe_df = users_ohe_df.set_index(\"uid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSM starter's pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем вектор юзера (anchor) и векторы двух айтемов - \"хорошего\" и \"плохого\" (positive и negative). Хороший айтем - это тот, который пользователь уже посмотрел, а в качестве плохого возьмем любой случайный айтем из датасета. Затем посчитаем расстояния:\n",
    "1. между вектором юзера и вектором \"хорошего\" айтема\n",
    "2. между вектором юзера и вектором \"плохого\" айтема\n",
    "\n",
    "_Значением функции потерь будет разность между первым и вторым расстоянием._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, n_dims=128, alpha=0.4):\n",
    "    # будем ожидать, что на вход функции прилетит три сконкатенированных \n",
    "    # вектора - вектор юзера и два вектора айтема\n",
    "    anchor = y_pred[:, 0:n_dims]\n",
    "    positive = y_pred[:, n_dims:n_dims*2]\n",
    "    negative = y_pred[:, n_dims*2:n_dims*3]\n",
    "\n",
    "    # считаем расстояния от вектора юзера до вектора хорошего айтема\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "    # и до плохого\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "\n",
    "    # считаем лосс\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = K.maximum(basic_loss, 0.0) # возвращаем ноль, если лосс отрицательный\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(items, users, interactions, batch_size=1024):\n",
    "    while True:\n",
    "        uid_meta = []\n",
    "        uid_interaction = []\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for _ in range(batch_size):\n",
    "            # берем рандомный uid\n",
    "            uid_i = randint(0, interactions.shape[0]-1)\n",
    "            # id хорошего айтема\n",
    "            pos_i = np.random.choice(range(interactions.shape[1]), p=interactions[uid_i])\n",
    "            # id плохого айтема\n",
    "            neg_i = np.random.choice(range(interactions.shape[1]))\n",
    "            # фичи юзера\n",
    "            uid_meta.append(users.iloc[uid_i])\n",
    "            # вектор айтемов, с которыми юзер взаимодействовал\n",
    "            uid_interaction.append(interactions_vec[uid_i])\n",
    "            # фичи хорошего айтема\n",
    "            pos.append(items.iloc[pos_i])\n",
    "            # фичи плохого айтема\n",
    "            neg.append(items.iloc[neg_i])\n",
    "            \n",
    "        yield [np.array(uid_meta), np.array(uid_interaction), np.array(pos), np.array(neg)], [np.array(uid_meta), np.array(uid_interaction)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вектор фичей юзера: (1024, 8)\n",
      "вектор взаимодействий юзера с айтемами: (1024, 3963)\n",
      "вектор 'хорошего' айтема: (1024, 1302)\n",
      "вектор 'плохого' айтема: (1024, 1302)\n",
      "\n",
      "вектор фичей юзера: (1024, 8)\n",
      "вектор взаимодействий юзера с айтемами: (1024, 3963)\n"
     ]
    }
   ],
   "source": [
    "# инициализируем генератор\n",
    "gen = generator(items=items_ohe_df.drop([\"item_id\"], axis=1), \n",
    "                users=users_ohe_df.drop([\"user_id\"], axis=1), \n",
    "                interactions=interactions_vec)\n",
    "\n",
    "ret = next(gen)\n",
    "\n",
    "\n",
    "print(f\"вектор фичей юзера: {ret[0][0].shape}\")\n",
    "print(f\"вектор взаимодействий юзера с айтемами: {ret[0][1].shape}\")\n",
    "print(f\"вектор 'хорошего' айтема: {ret[0][2].shape}\")\n",
    "print(f\"вектор 'плохого' айтема: {ret[0][3].shape}\")\n",
    "print()\n",
    "print(f\"вектор фичей юзера: {ret[1][0].shape}\")\n",
    "print(f\"вектор взаимодействий юзера с айтемами: {ret[1][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы обучить модель используя триплет лосс, нам нужно получить три вектора - вектор юзера, вектор \"хорошего\" айтема и вектор \"плохого\" айтема. Для этого нам нужно две модели (\"хороший\" и \"плохой\" айтем будут семплироваться одной и той же моделью).  \n",
    "\n",
    "Модель юзера будет иметь два входа:\n",
    "- вход для фичей юзера (фичи из users_ohe_df)\n",
    "- вход для вектора айтемов, которые посмотрел юзер (строка interactions_vec, которая соответствует uid конкретного юзера)\n",
    "\n",
    "Выход модели юзера будет размерностью N_FACTORS.\n",
    "\n",
    "У модели айтема будет один вход для фичей айтема (из items_ohe_df) и один выход также размерностью N_FACTORS.\n",
    "\n",
    "Общая архитектура будет вот такой: \n",
    "- есть модель юзера и модель айтема\n",
    "- обе модели семплируют юзер и айтем-фичи во внутреннее пространство размерностью N_FACTORS\n",
    "- модель айтема семплирует два айтема - \"хороший\" и \"плохой\"\n",
    "- в итоге получается три вектора размерностью N_FACTORS (вектор юзера, вектор \"хорошего\" айтема и вектор \"плохого\" айтема)\n",
    "- затем полученные векторы конкатенируются, по ним считается triplet loss\n",
    "- profit\n",
    "\n",
    "Для того, чтобы собрать модель, помимо размерности внутреннего пространства, нам нужно знать еще размерность вектора юзера и вектора айтема. Зададим их сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_FACTORS: 128\n",
      "ITEM_MODEL_SHAPE: (1302,)\n",
      "USER_META_MODEL_SHAPE: (8,)\n",
      "USER_INTERACTION_MODEL_SHAPE: (3963,)\n"
     ]
    }
   ],
   "source": [
    "N_FACTORS = 128\n",
    "\n",
    "# в датасетах есть столбец user_id/item_id, помним, что он не является фичей для обучения!\n",
    "ITEM_MODEL_SHAPE = (items_ohe_df.drop([\"item_id\"], axis=1).shape[1], ) \n",
    "USER_META_MODEL_SHAPE = (users_ohe_df.drop([\"user_id\"], axis=1).shape[1], )\n",
    "\n",
    "USER_INTERACTION_MODEL_SHAPE = (interactions_vec.shape[1], )\n",
    "\n",
    "print(f\"N_FACTORS: {N_FACTORS}\")\n",
    "print(f\"ITEM_MODEL_SHAPE: {ITEM_MODEL_SHAPE}\")\n",
    "print(f\"USER_META_MODEL_SHAPE: {USER_META_MODEL_SHAPE}\")\n",
    "print(f\"USER_INTERACTION_MODEL_SHAPE: {USER_INTERACTION_MODEL_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_model(n_factors=N_FACTORS):\n",
    "    # входной слой\n",
    "    inp = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "    \n",
    "    # полносвязный слой\n",
    "    layer_1 = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                               kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                               activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp)\n",
    "\n",
    "    # делаем residual connection - складываем два слоя, \n",
    "    # чтобы градиенты не затухали во время обучения\n",
    "    layer_2 = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(layer_1)\n",
    "    \n",
    "    add = keras.layers.Add()([layer_1, layer_2])\n",
    "    \n",
    "    # выходной слой\n",
    "    out = keras.layers.Dense(N_FACTORS, activation='linear', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(add)\n",
    "    \n",
    "    return keras.models.Model(inp, out)\n",
    "\n",
    "\n",
    "def user_model(n_factors=N_FACTORS):\n",
    "    # входной слой для вектора фичей юзера (из users_ohe_df)\n",
    "    inp_meta = keras.layers.Input(shape=USER_META_MODEL_SHAPE)\n",
    "    # входной слой для вектора просмотров (из iteractions_vec)\n",
    "    inp_interaction = keras.layers.Input(shape=USER_INTERACTION_MODEL_SHAPE)\n",
    "\n",
    "    # полносвязный слой\n",
    "    layer_1_meta = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp_meta)\n",
    "\n",
    "    layer_1_interaction = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(inp_interaction)\n",
    "\n",
    "    # делаем residual connection - складываем два слоя,\n",
    "    # чтобы градиенты не затухали во время обучения\n",
    "    layer_2_meta = keras.layers.Dense(N_FACTORS, activation='elu', use_bias=False,\n",
    "                                 kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                                 activity_regularizer=keras.regularizers.l2(l2=1e-6))(layer_1_meta)\n",
    "    \n",
    "\n",
    "    add = keras.layers.Add()([layer_1_meta, layer_2_meta])\n",
    "    \n",
    "    # конкатенируем вектор фичей с вектором просмотров\n",
    "    concat_meta_interaction = keras.layers.Concatenate()([add, layer_1_interaction])\n",
    "    \n",
    "    # выходной слой\n",
    "    out = keras.layers.Dense(N_FACTORS, activation='linear', use_bias=False,\n",
    "                             kernel_regularizer=keras.regularizers.l2(1e-6),\n",
    "                             activity_regularizer=keras.regularizers.l2(l2=1e-6))(concat_meta_interaction)\n",
    "    \n",
    "    return keras.models.Model([inp_meta, inp_interaction], out)\n",
    "\n",
    "# инициализируем модели юзера и айтема\n",
    "i2v = item_model()\n",
    "u2v = user_model()\n",
    "\n",
    "# вход для вектора фичей юзера (из users_ohe_df)\n",
    "ancor_meta_in = keras.layers.Input(shape=USER_META_MODEL_SHAPE)\n",
    "# вход для вектора просмотра юзера (из interactions_vec)\n",
    "ancor_interaction_in = keras.layers.Input(shape=USER_INTERACTION_MODEL_SHAPE)\n",
    "\n",
    "# вход для вектора \"хорошего\" айтема\n",
    "pos_in = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "# вход для вектора \"плохого\" айтема\n",
    "neg_in = keras.layers.Input(shape=ITEM_MODEL_SHAPE)\n",
    "\n",
    "# получаем вектор юзера\n",
    "ancor = u2v([ancor_meta_in, ancor_interaction_in])\n",
    "# получаем вектор \"хорошего\" айтема\n",
    "pos = i2v(pos_in)\n",
    "# получаем вектор \"плохого\" айтема\n",
    "neg = i2v(neg_in)\n",
    "\n",
    "# конкатенируем полученные векторы\n",
    "res = keras.layers.Concatenate(name=\"concat_ancor_pos_neg\")([ancor, pos, neg])\n",
    "\n",
    "# собираем модель\n",
    "model = keras.models.Model([ancor_meta_in, ancor_interaction_in, pos_in, neg_in], res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'recsys_resnet_linear'\n",
    "\n",
    "# логируем процесс обучения в тензорборд\n",
    "t_board = keras.callbacks.TensorBoard(log_dir=f'runs/{model_name}')\n",
    "\n",
    "# уменьшаем learning_rate, если лосс долго не уменьшается (в течение двух эпох)\n",
    "decay = keras.callbacks.ReduceLROnPlateau(monitor='loss', patience=2, factor=0.8, verbose=1)\n",
    "\n",
    "# сохраняем модель после каждой эпохи, если лосс уменьшился\n",
    "check = keras.callbacks.ModelCheckpoint(filepath=model_name + '/epoch{epoch}-{loss:.2f}.h5', monitor=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# компилируем модель, используем оптимайзер Adam и triplet loss\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss=triplet_loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 1302)]               0         []                            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  166656    ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 128)                  16384     ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 128)                  0         ['dense_7[0][0]',             \n",
      "                                                                     'dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 128)                  16384     ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 199424 (779.00 KB)\n",
      "Trainable params: 199424 (779.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# модель айтема\n",
    "item_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 128)                  1024      ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 128)                  16384     ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 3963)]               0         []                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 128)                  0         ['dense_10[0][0]',            \n",
      "                                                                     'dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 128)                  507264    ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 256)                  0         ['add_3[0][0]',               \n",
      " )                                                                   'dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 128)                  32768     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 557440 (2.13 MB)\n",
      "Trainable params: 557440 (2.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# модель юзера\n",
    "user_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 3963)]               0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, 1302)]               0         []                            \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 1302)]               0         []                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, 128)                  557440    ['input_4[0][0]',             \n",
      "                                                                     'input_5[0][0]']             \n",
      "                                                                                                  \n",
      " model (Functional)          (None, 128)                  199424    ['input_6[0][0]',             \n",
      "                                                                     'input_7[0][0]']             \n",
      "                                                                                                  \n",
      " concat_ancor_pos_neg (Conc  (None, 384)                  0         ['model_1[0][0]',             \n",
      " atenate)                                                            'model[0][0]',               \n",
      "                                                                     'model[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 756864 (2.89 MB)\n",
      "Trainable params: 756864 (2.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# общая модель\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 9s 79ms/step - loss: 0.3002 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.2261 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.1910 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 0.1733 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.1507 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.1475 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.1342 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.1304 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.1204 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.1166 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.1149 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.1030 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 0.1118 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.1074 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.0981 - lr: 8.0000e-04\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.0957 - lr: 8.0000e-04\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.0904 - lr: 8.0000e-04\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.0921 - lr: 8.0000e-04\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.0880 - lr: 8.0000e-04\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.0820 - lr: 8.0000e-04\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.0864 - lr: 8.0000e-04\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0853\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.0853 - lr: 8.0000e-04\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.0803 - lr: 6.4000e-04\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.0792 - lr: 6.4000e-04\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.0736 - lr: 6.4000e-04\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.0773 - lr: 6.4000e-04\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0768\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.0768 - lr: 6.4000e-04\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 0.0754 - lr: 5.1200e-04\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0738\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.0738 - lr: 5.1200e-04\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.0733 - lr: 4.0960e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18535f93e50>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# начинаем обучение, не забывая дропнуть столбцы item_id и user_id \n",
    "# из датафреймов при инициализации генератора.\n",
    "\n",
    "# batch_size можно (и лучше) поставить побольше, если вы не органичены в ресурсах\n",
    "\n",
    "model.fit(generator(items=items_ohe_df.drop([\"item_id\"], axis=1), \n",
    "                    users=users_ohe_df.drop([\"user_id\"], axis=1), \n",
    "                    interactions=interactions_vec,\n",
    "                    batch_size=64), \n",
    "          steps_per_epoch=100, \n",
    "          epochs=30, \n",
    "          initial_epoch=0,\n",
    "          callbacks=[decay, t_board, check]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Отлично! Мы подготовили данные, собрали модель по архитектуре DSSM и обучили ее.\n",
    "Теперь возьмем случайного юзера и случайный айтем. Как понять, насколько этот айтем релевантен юзеру?\n",
    "\n",
    "Нужно:\n",
    "- получить вектор айтема;\n",
    "- получить вектор юзера;\n",
    "- посчитать расстояние между ними.\n",
    "\n",
    "Это расстояние и есть мера релевантности.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 287ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5424578]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# берем рандомного юзера\n",
    "rand_uid = np.random.choice(list(users_ohe_df.index))\n",
    "\n",
    "# получаем фичи юзера и вектор его просмотров айтемов\n",
    "user_meta_feats = users_ohe_df.drop([\"user_id\"], axis=1).iloc[rand_uid.astype(int)]\n",
    "user_interaction_vec = interactions_vec[rand_uid.astype(int)]\n",
    "\n",
    "# берем рандомный айтем\n",
    "rand_iid = np.random.choice(list(items_ohe_df.index))\n",
    "# получаем фичи айтема\n",
    "item_feats = items_ohe_df.drop([\"item_id\"], axis=1).iloc[rand_iid]\n",
    "\n",
    "# получаем вектор юзера\n",
    "user_vec = u2v.predict([np.array(user_meta_feats).reshape(1, -1), \n",
    "                        np.array(user_interaction_vec).reshape(1, -1)])\n",
    "\n",
    "# и вектор айтема\n",
    "item_vec = i2v.predict(np.array(item_feats).reshape(1, -1))\n",
    "\n",
    "# считаем расстояние между вектором юзера и вектором айтема\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ED\n",
    "\n",
    "ED(user_vec, item_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# получаем фичи всех айтемов\n",
    "items_feats = items_ohe_df.drop([\"item_id\"], axis=1).to_numpy()\n",
    "# получаем векторы всех айтемов\n",
    "items_vecs = i2v.predict(items_feats)\n",
    "\n",
    "# считаем расстояния\n",
    "dists = ED(user_vec, items_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2935, 1510, 1822,   87, 2664, 1399, 1455, 1911,  369, 3151, 1089,\n",
       "       2211, 1977,  238, 2326, 2768, 3533, 1522, 1351, 3099], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_iids = np.argsort(dists, axis=1)[0][:20]\n",
    "top5_iids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось конвертировать внутренние iid в ~~id здорового человека~~ item_id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5559,\n",
       " 2875,\n",
       " 3461,\n",
       " 177,\n",
       " 5042,\n",
       " 2679,\n",
       " 2775,\n",
       " 3637,\n",
       " 741,\n",
       " 5956,\n",
       " 2074,\n",
       " 4178,\n",
       " 3759,\n",
       " 484,\n",
       " 4370,\n",
       " 5256,\n",
       " 6663,\n",
       " 2893,\n",
       " 2590,\n",
       " 5876]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_item_ids = [iid_to_item_id[iid] for iid in top5_iids]\n",
    "top5_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177                       Аллея кошмаров\n",
       "484                    Аватар: Путь воды\n",
       "741                            Под водой\n",
       "2074                             Морбиус\n",
       "2590                         Оленьи рога\n",
       "2679                              Вечныe\n",
       "2775                     Человек на Луне\n",
       "2875                      Смерть на Ниле\n",
       "2893                    Не говори никому\n",
       "3461                     Последняя дуэль\n",
       "3637                Прошлой ночью в Сохо\n",
       "3759                    Дьявол в деталях\n",
       "4178              Бегущий по лезвию 2049\n",
       "4370      Убийство в Восточном экспрессе\n",
       "5042              Хроники хищных городов\n",
       "5256                        К югу от рая\n",
       "5559                                Дюна\n",
       "5876    Черная пантера: Ваканда навсегда\n",
       "5956                  King’s Man: Начало\n",
       "6663       Клаустрофобы 2: Лига выживших\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_titles = items_df.loc[items_df.item_id.isin(top5_item_ids)].name\n",
    "recommended_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
